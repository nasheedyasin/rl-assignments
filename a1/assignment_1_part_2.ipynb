{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Stocastic Grid Env\n",
    "\n",
    "### Rules:\n",
    "- Landing on a square causes the reward on that square to be consumed. i.e. landing on that square again will yield no reward (0 reward).\n",
    "- Whenever you perform an action there is a 2/3rd chance that the action is as you expect it to be, a 2/9th chance that it is the double of what you want it to be and a 1/9th chance of it being a mirror action.\n",
    "- The reward (and negative reward) for each state is distributed in a reciprocal fashion. i.e. 1/9th the reward when coming through the expected action, 2/9th reward when coming via the opposite action and 2/3rd reward when coming via a mirror action. \n",
    "\n",
    "For instance on a 5x5 grid where the reward on each square is 1 (for simplicity):\n",
    "\n",
    "If `s = (2,3)` and `a = RIGHT`:\n",
    "-   2/3rd chance `s' = (2,4)` and `r = 1/9`\n",
    "-   2/9th chance `s' = (2,5)` and `r = 2/9`\n",
    "-   2/3rd chance `s' = (3,2)` and `r = 2/3`\n",
    "\n",
    "Exception: Landing on the `goal` square will always yield a maximal reward (of 1 for the above example) regardless of how you get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "        \"goal_img\": \"./goal.png\",\n",
    "        \"agent_img\": \"./agent.png\",\n",
    "        \"reward_img\": \"./reward.png\",\n",
    "        \"neg_reward_img\": \"./neg_reward.png\" \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        max_time_steps:int = 20,\n",
    "        render_mode=None\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), dtype=int),\n",
    "                \"goal\": spaces.Box(0, size - 1, shape=(2, ), dtype=int),\n",
    "            }\n",
    "        )\n",
    "        self.observation_space['agent'].sample()\n",
    "        self.observation_space['goal'].sample()\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_mode_space = spaces.Discrete(3)\n",
    "        self.max_timesteps = max_time_steps\n",
    "\n",
    "        # Randomly distribute rewards between -0.75 to 0.75\n",
    "        self._base_state = self.np_random.uniform(\n",
    "            -.75, .75, size=(self.size, self.size)\n",
    "        )\n",
    "        np.around(self._base_state, 2, self._base_state)\n",
    "\n",
    "        \"\"\"The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: {'move': np.array([1, 0]), 'name': 'down'},\n",
    "            1: {'move': np.array([-1, 0]), 'name': 'up'},\n",
    "            2: {'move': np.array([0, 1]), 'name': 'right'},\n",
    "            3: {'move': np.array([0, -1]), 'name': 'left'},\n",
    "        }\n",
    "        self._action_mode_mapping = {\n",
    "            0: {'p': 2/3, 'name': 'as-is', 'score_mul': 1/9},\n",
    "            1: {'p': 2/9, 'name': 'double', 'score_mul': 2/9},\n",
    "            2: {'p': 1/9, 'name': 'mirror', 'score_mul': 2/3}\n",
    "        }\n",
    "\n",
    "        self.timestep = 0\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        self._goal_pos = self._agent_pos.copy()\n",
    "\n",
    "        # We will sample the goal's location randomly until it does not coincide\n",
    "        # with the agent's location\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._goal_pos = self.observation_space['goal'].sample()\n",
    "\n",
    "        self.state = self._base_state.copy()\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        # Check for render mode legality\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.window_size = 744  # The size of the PyGame window\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset env\n",
    "        self.state = self._base_state.copy()\n",
    "\n",
    "        # Agent start pos changes everytime\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Randomly select the action mode\n",
    "        action_mode = self.np_random.choice(\n",
    "            range(self.action_mode_space.n),\n",
    "            p=[self._action_mode_mapping[i]['p']\n",
    "               for i in range(self.action_mode_space.n)]\n",
    "        )\n",
    "\n",
    "        if self._action_mode_mapping[action_mode]['name'] in ('opposite', 'as-is'):\n",
    "            direction = self._action_to_direction[action]['move'].copy()\n",
    "            if self._action_mode_mapping[action_mode]['name'] == 'opposite':\n",
    "                direction *= 2\n",
    "\n",
    "            # Restricting the agent to the grid.\n",
    "            self._agent_pos = np.clip(\n",
    "                self._agent_pos + direction, 0, self.size - 1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self._agent_pos = self._agent_pos[::-1]\n",
    "\n",
    "        # Record the action taken\n",
    "        observation = self._get_obs()\n",
    "        observation.update({\n",
    "            'action': self._action_to_direction[action]['name'],\n",
    "            'action_mode': self._action_mode_mapping[action_mode]['name']\n",
    "        })\n",
    "\n",
    "        # Check if terminated\n",
    "        if np.array_equal(self._agent_pos, self._goal_pos):\n",
    "            reward = 1.\n",
    "            terminated = True\n",
    "\n",
    "        else:\n",
    "            reward = np.round(self.state[tuple(self._agent_pos)] * \n",
    "                self._action_mode_mapping[action_mode]['score_mul'], 2)\n",
    "            terminated = False\n",
    "\n",
    "        # Consume reward\n",
    "        self.state[tuple(self._agent_pos)] = 0        \n",
    "        \n",
    "        # An episode is done iff the agent has reached the goal\n",
    "        self.timestep += 1\n",
    "\n",
    "        truncated = self.timestep > self.max_timesteps\n",
    "\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_pos, \"goal\": self._goal_pos}\n",
    "\n",
    "    # City block distance between goal and agent\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_pos - self._goal_pos, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # Goal image\n",
    "        goal_img = pygame.image.load(self.metadata['goal_img'])\n",
    "        goal_img = pygame.transform.scale(goal_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            goal_img,\n",
    "            pix_square_size * self._goal_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Agent image\n",
    "        agent_img = pygame.image.load(self.metadata['agent_img'])\n",
    "        agent_img = pygame.transform.scale(agent_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            agent_img,\n",
    "            pix_square_size * self._agent_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Reward image\n",
    "        reward_img = pygame.image.load(self.metadata['reward_img'])\n",
    "        # Negative reward image\n",
    "        neg_reward_img = pygame.image.load(self.metadata['neg_reward_img'])\n",
    "\n",
    "        # Add the reward and neg reward\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                reward = self.state[x, y]\n",
    "                if self.state[x, y] > 0 and self.state[x, y] < 1:\n",
    "                    sreward_img = pygame.transform.scale(\n",
    "                        reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    rew_sz = np.array(sreward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= rew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sreward_img,\n",
    "                        position\n",
    "                    )\n",
    "                elif self.state[x, y] < 0:\n",
    "                    reward *= -1\n",
    "                    sneg_reward_img = pygame.transform.scale(\n",
    "                        neg_reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    nrew_sz = np.array(sneg_reward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= nrew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sneg_reward_img,\n",
    "                        position\n",
    "                    )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "    self.observation_space = env.observation_space\n",
    "    self.action_space = env.action_space\n",
    "\n",
    "  def step(self, obs):\n",
    "    return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': array([0, 2]), 'goal': array([5, 2])}\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'left', 'action_mode': 'as-is'} -0.04\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 2]), 'goal': array([5, 2]), 'action': 'right', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'left', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([1, 0]), 'goal': array([5, 2]), 'action': 'left', 'action_mode': 'double'} -0.05\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'double'} 0.0\n",
      "{'agent': array([1, 0]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'mirror'} 0.0\n",
      "{'agent': array([1, 0]), 'goal': array([5, 2]), 'action': 'left', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([1, 0]), 'goal': array([5, 2]), 'action': 'left', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'down', 'action_mode': 'mirror'} 0.0\n",
      "{'agent': array([1, 0]), 'goal': array([5, 2]), 'action': 'right', 'action_mode': 'double'} 0.0\n",
      "{'agent': array([0, 0]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'as-is'} -0.04\n",
      "{'agent': array([0, 0]), 'goal': array([5, 2]), 'action': 'down', 'action_mode': 'mirror'} 0.0\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'right', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 2]), 'goal': array([5, 2]), 'action': 'right', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'left', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([1, 1]), 'goal': array([5, 2]), 'action': 'down', 'action_mode': 'as-is'} -0.03\n",
      "{'agent': array([0, 1]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([0, 2]), 'goal': array([5, 2]), 'action': 'right', 'action_mode': 'as-is'} 0.0\n",
      "{'agent': array([2, 0]), 'goal': array([5, 2]), 'action': 'up', 'action_mode': 'double'} 0.04\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = GridEnvironment(size=6, render_mode='human')\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(obs)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "env.render()\n",
    "while not (terminated or truncated):\n",
    "  action = agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  env.render()\n",
    "  print(obs, reward)\n",
    "  time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('expt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
