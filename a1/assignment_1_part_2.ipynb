{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pygame\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any\n",
    "\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display set up\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import hashlib\n",
    "import matplotlib.pyplot as plt\n",
    "from itertools import combinations\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "  def __init__(self, env: Env, dynamic: bool = False, doubleq: bool = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      env (Env): The environment the agent should act in.\n",
    "      doubleq (bool): Whether to use double Q-Learning.\n",
    "    \"\"\"\n",
    "    # Intialize Q-Table\n",
    "    self.env = env\n",
    "    self.dynamic = dynamic\n",
    "    self.doubleq = doubleq\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # To allow usage of this agent with the stock trading env\n",
    "    if isinstance(env.observation_space, spaces.Dict):\n",
    "      num_states = len(env.states.flatten())\n",
    "      if dynamic:\n",
    "        num_states *= self._get_num_situations(env.states.count_non_zero())\n",
    "        self._indexer(num_states)\n",
    "    else:\n",
    "      num_states = env.observation_space.n\n",
    "\n",
    "    # Intialize 2 Q tables if `doubleq` is True\n",
    "    if doubleq: q_table_shape = (2, num_states, num_actions)\n",
    "    else: q_table_shape = (num_states, num_actions)\n",
    "\n",
    "    self._q_table = np.zeros((\n",
    "      q_table_shape\n",
    "    ))   \n",
    "\n",
    "  @classmethod\n",
    "  def _get_num_situations(cls, rewards):\n",
    "    # Returns game situation count for games with disappearing rewards\n",
    "    # and goals that can only be reached after all the rewards are collected.\n",
    "    situations = 0\n",
    "    for i in range(1, rewards+1):\n",
    "        situations += math.comb(rewards,i)\n",
    "    return situations\n",
    "\n",
    "  def _indexer(self, num_states):\n",
    "    states = self.env.states.flatten()\n",
    "    rwd_indices = np.flatnonzero(states).tolist()\n",
    "    goal_pos = self.env._goal_pos[0] * self.env.size + self.env._goal_pos[1]\n",
    "\n",
    "    q_table_indices = list(range(num_states))\n",
    "\n",
    "    self._index_map = {}\n",
    "    for pos in tqdm(range(len(states)), desc='Indexing States'):\n",
    "      # Loop over all the situations\n",
    "      for r in range(1, states.count_non_zero()):\n",
    "        for comb in combinations(rwd_indices, r):\n",
    "          for indices in comb:\n",
    "            # Define the situation\n",
    "            sit = states.copy()\n",
    "            for idx in indices:\n",
    "              if idx == goal_pos: sit[idx] = self.env.observation_space['reward'].high[0]\n",
    "              else: sit[idx] = 0.\n",
    "  \n",
    "            m = hashlib.sha256()\n",
    "            m.update(bytes(pos)+sit.tobytes())\n",
    "            # Map a q_table state to the situation\n",
    "            self._index_map[m.hexdigest()] = q_table_indices.pop(0)\n",
    "\n",
    "\n",
    "  @property\n",
    "  def q_table(self) -> np.ndarray:\n",
    "    return self._q_table\n",
    "\n",
    "  @q_table.setter\n",
    "  def q_table(self, path_to_q_table: str):\n",
    "    with open(path_to_q_table, 'rb') as f:\n",
    "      q_table = pickle.load(f)\n",
    "\n",
    "    # Check if the table is compatible\n",
    "    if self.q_table.shape != q_table.shape:\n",
    "      raise ValueError(\"Trying to load incompatible Q-Table.\")\n",
    "\n",
    "    self._q_table = q_table\n",
    "\n",
    "  def greedy_policy(self, state: int):\n",
    "    if self.doubleq:\n",
    "      # Choose the best action after taking the mean value of\n",
    "      # the actions accross both tables.\n",
    "      return self.q_table.mean(axis=0)[state].argmax()\n",
    "\n",
    "    else: return self.q_table[state].argmax()\n",
    "\n",
    "  def ep_greedy_policy(self, state, epsilon):\n",
    "    random_num = self.env.np_random.uniform(0, 1)\n",
    "\n",
    "    if random_num > epsilon:\n",
    "      return self.greedy_policy(state)\n",
    "\n",
    "    else:\n",
    "      return self.env.action_space.sample()\n",
    "\n",
    "  def _get_state_idx(self, observation: Any) -> int:\n",
    "    \"\"\"Helper func to allow the same Agent to be used with the stock\n",
    "    prediction env.\n",
    "    \"\"\"\n",
    "    if isinstance(observation, dict): state = observation['agent']\n",
    "    else: state = observation\n",
    "\n",
    "    # Convert 2-d state index to flat index\n",
    "    # flat_idx = curr_row*num_cols+curr_col\n",
    "    if isinstance(state, np.ndarray): \n",
    "      state = state[0] * self.env.size + state[1]\n",
    "    else:\n",
    "      return state\n",
    "\n",
    "    if not self.dynamic: return state\n",
    "\n",
    "    # Get dynamic index\n",
    "    sit = observation['reward']\n",
    "    m = hashlib.sha256()\n",
    "    m.update(bytes(state)+sit.tobytes())\n",
    "    return self._index_map[m.hexdigest()]\n",
    "\n",
    "  def step(self, observation):\n",
    "    state = self._get_state_idx(observation)\n",
    "    return self.greedy_policy(state)\n",
    "\n",
    "  def save_pretrained(self, path_to_q_table: str):\n",
    "    with open(path_to_q_table, 'wb') as f:\n",
    "      pickle.dump(self.q_table, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTrainer:\n",
    "    def __init__(self, agent: QLearningAgent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_episodes: int = 10**5,\n",
    "        lr: float = .4,\n",
    "        gamma: float = .9,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = 1.,\n",
    "        decay_rate: float = .0008\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 10**5.\n",
    "        lr (float, optional): The learning rate. Defaults to .4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to .0008.\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            state = self.agent._get_state_idx(obs)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                action = self.agent.ep_greedy_policy(state, epsilon)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "                new_state = self.agent._get_state_idx(obs)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the Q table\n",
    "                next_state_qvalue = self.agent.q_table[new_state].max()\n",
    "\n",
    "                self.agent.q_table[state, action] += lr * \\\n",
    "                (reward+gamma*next_state_qvalue-self.agent.q_table[state, action])\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n",
    "\n",
    "    def evaluate(self, num_episodes: int = 10**2) -> List[float]:\n",
    "        \"\"\"Evaluate the agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): Defaults to 10**2.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                action = self.agent.step(obs)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQTrainer(QLearningTrainer):\n",
    "    def __init__(self, agent: QLearningAgent):\n",
    "        assert agent.doubleq, \"The agent passed isn't configured for Double Q \"\\\n",
    "            \"learning\"\n",
    "        super().__init__(agent)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_episodes: int = 10**5,\n",
    "        lr: float = .4,\n",
    "        gamma: float = .9,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = 1.,\n",
    "        decay_rate: float = .0008\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 10**5.\n",
    "        lr (float, optional): The learning rate. Defaults to .4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to .0008.\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            state = self.agent._get_state_idx(obs)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                action = self.agent.ep_greedy_policy(state, epsilon)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "                new_state = self.agent._get_state_idx(obs)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                # Choose the table to update\n",
    "                upd_idx = self.agent.env.np_random.choice([0, 1])\n",
    "                eval_idx = 1-upd_idx\n",
    "\n",
    "                # Update the Q table\n",
    "                next_state_qvalue = self.agent.q_table[\n",
    "                    eval_idx,\n",
    "                    new_state,\n",
    "                    self.agent.q_table[upd_idx, new_state].argmax()\n",
    "                ]\n",
    "\n",
    "                self.agent.q_table[upd_idx, state, action] += lr * \\\n",
    "                (reward+gamma*next_state_qvalue-self.agent.q_table[upd_idx, state, action])\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Determinisitic Grid Env\n",
    "\n",
    "### Rules:\n",
    "- Each square has a randomly assigned reward between -0.5 to 0.5. The goal has a reward of 1.0\n",
    "- The size of the cactus/lemonade is indicative of the reward.\n",
    "- Landing on a square causes the reward on that square to be consumed. i.e. landing on that square again will yield no reward (0 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicGridEnvironment(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "        \"background_img\": \"./assets/background.jpg\",\n",
    "        \"goal_img\": \"./assets/goal.png\",\n",
    "        \"agent_img\": \"./assets/agent.png\",\n",
    "        \"reward_img\": \"./assets/reward.png\",\n",
    "        \"neg_reward_img\": \"./assets/neg_reward.png\" \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        max_time_steps:int = 20,\n",
    "        render_mode=None,\n",
    "        seed=88\n",
    "    ):\n",
    "        # Seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                \"goal\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                # Randomly distribute rewards between -.5 to .5\n",
    "                # (have an initial upperbound of 1. as that is the goal reward, then set the\n",
    "                # non goal rewards > .5 as .5)\n",
    "                \"reward\": spaces.Box(\n",
    "                    -1., 1.,\n",
    "                    shape=(size**2, ),\n",
    "                    seed=self.np_random,\n",
    "                    dtype=np.float16\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_mode_space = spaces.Discrete(3)\n",
    "        self.max_timesteps = max_time_steps\n",
    "\n",
    "        self._base_states = np.zeros((size, size), dtype=np.float16)\n",
    "\n",
    "        # Add 2 negative rewards\n",
    "        for _ in range(8):\n",
    "            neg_reward_loc = self.observation_space['agent'].sample()\n",
    "            while self._base_states[tuple(neg_reward_loc)] != 0:\n",
    "                neg_reward_loc = self.observation_space['agent'].sample()\n",
    "\n",
    "            self._base_states[tuple(neg_reward_loc)] = self.observation_space['reward'].low[0]\n",
    "\n",
    "        # Add 4 positive rewards\n",
    "        for _ in range(0):\n",
    "            pos_reward_loc = self.observation_space['agent'].sample()\n",
    "            while self._base_states[tuple(pos_reward_loc)] != 0:\n",
    "                pos_reward_loc = self.observation_space['agent'].sample()\n",
    "\n",
    "            self._base_states[tuple(pos_reward_loc)] = \\\n",
    "                .5 * self.observation_space['reward'].high[0]\n",
    "\n",
    "        \"\"\"The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: {'move': np.array([1, 0]), 'name': 'down'},\n",
    "            1: {'move': np.array([-1, 0]), 'name': 'up'},\n",
    "            2: {'move': np.array([0, 1]), 'name': 'right'},\n",
    "            3: {'move': np.array([0, -1]), 'name': 'left'},\n",
    "        }\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.states = self._base_states.copy()\n",
    "        # We will sample the goals's location randomly until it does not coincide\n",
    "        # with the reward locations.\n",
    "        self._goal_pos = self.observation_space['goal'].sample()\n",
    "        while self.states[tuple(self._goal_pos)] != 0:\n",
    "            self._goal_pos = self.observation_space['goal'].sample()\n",
    "\n",
    "        self.states[tuple(self._goal_pos)] = self.observation_space['reward'].low[0]\n",
    "\n",
    "        # We will sample the agent's location randomly until it does not coincide\n",
    "        # with the goal and reward locations.\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while self.states[tuple(self._agent_pos)] != 0:\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        # Check for render mode legality\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.window_size = 744  # The size of the PyGame window\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # For compliance\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset env\n",
    "        self.timestep = 0\n",
    "        self.states = self._base_states.copy()\n",
    "        self.states[tuple(self._goal_pos)] = self.observation_space['reward'].low[0]\n",
    "\n",
    "        # Agent start pos changes everytime\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while self.states[tuple(self._agent_pos)] != 0:\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        direction = self._action_to_direction[action]['move']\n",
    "        \n",
    "        # Restricting the agent to the grid.\n",
    "        new_pos = np.clip(\n",
    "            self._agent_pos + direction,\n",
    "            0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # Give negative reward for actions that keep you in the same\n",
    "        # place (boundary actions)\n",
    "        if np.array_equal(self._agent_pos, new_pos):\n",
    "            reward = self.observation_space['reward'].low[0]\n",
    "        else: reward = self.states[tuple(new_pos)]\n",
    "\n",
    "        # In all cases add a reward penalty of max negative reward\n",
    "        # in other words, every action is penalized.\n",
    "        reward += self.observation_space['reward'].low[0]\n",
    "\n",
    "        self._agent_pos = new_pos\n",
    "\n",
    "        # Record the action taken\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        # Consume reward\n",
    "        self.states[tuple(self._agent_pos)] = 0        \n",
    "        \n",
    "        # An episode is done iff the agent has reached the goal\n",
    "        self.timestep += 1\n",
    "\n",
    "        truncated = self.timestep > self.max_timesteps\n",
    "\n",
    "        terminated = np.array_equal(new_pos, self._goal_pos)\n",
    "\n",
    "        info = self._get_info()\n",
    "        info.update({\n",
    "            'action': self._action_to_direction[action]['name'] \n",
    "        })\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_pos,\n",
    "            \"goal\": self._goal_pos,\n",
    "            \"reward\": self.states.flatten()\n",
    "        }\n",
    "\n",
    "    # City block distance between goal and agent\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_pos - self._goal_pos, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # Background image\n",
    "        bg_img = pygame.image.load(self.metadata['background_img'])\n",
    "        bg_img.set_alpha(188)\n",
    "        bg_img = pygame.transform.scale(bg_img, (self.window_size, self.window_size))\n",
    "        canvas.blit(\n",
    "            bg_img,\n",
    "            [0, 0]\n",
    "        )\n",
    "\n",
    "        # Goal image\n",
    "        goal_img = pygame.image.load(self.metadata['goal_img'])\n",
    "        goal_img = pygame.transform.scale(goal_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            goal_img,\n",
    "            pix_square_size * self._goal_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Agent image\n",
    "        agent_img = pygame.image.load(self.metadata['agent_img'])\n",
    "        agent_img = pygame.transform.scale(agent_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            agent_img,\n",
    "            pix_square_size * self._agent_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Reward image\n",
    "        reward_img = pygame.image.load(self.metadata['reward_img'])\n",
    "        # Negative reward image\n",
    "        neg_reward_img = pygame.image.load(self.metadata['neg_reward_img'])\n",
    "\n",
    "        # Add the reward and neg reward\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                reward = self.states[x, y]\n",
    "                if self.states[x, y] > 0 and self.states[x, y] < 1:\n",
    "                    sreward_img = pygame.transform.scale(\n",
    "                        reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    rew_sz = np.array(sreward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= rew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sreward_img,\n",
    "                        position\n",
    "                    )\n",
    "                elif self.states[x, y] < 0:\n",
    "                    reward *= -1\n",
    "                    sneg_reward_img = pygame.transform.scale(\n",
    "                        neg_reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    nrew_sz = np.array(sneg_reward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= nrew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sneg_reward_img,\n",
    "                        position\n",
    "                    )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent and the env\n",
    "denv = DeterministicGridEnvironment(size=6, render_mode='rgb_array', max_time_steps=40)\n",
    "\n",
    "# Check for API Compliance\n",
    "env_checker.check_env(denv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**4\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "qlearning_agent = QLearningAgent(denv)\n",
    "qtrainer = QLearningTrainer(qlearning_agent)\n",
    "tr_q_history = qtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "qlearning_agent.save_pretrained('./q-table-det.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# qlearning_agent.q_table = './q-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "step = int(num_episodes/50)\n",
    "plt.plot(range(0, num_episodes, step), tr_q_history[::step], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_q_history = qtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "plt.plot(range(0, num_eval_episodes, 25), ev_q_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_policy = [denv._action_to_direction[qlearning_agent.q_table[i].argmax()]['name']\n",
    "                  for i in range(qlearning_agent.q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((denv.size, denv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "obs, info = denv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(denv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = qlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = denv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(denv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "denv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Double Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**5\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "dqlearning_agent = QLearningAgent(denv, doubleq=True)\n",
    "dqtrainer = DoubleQTrainer(dqlearning_agent)\n",
    "tr_dq_history = dqtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "dqlearning_agent.save_pretrained('./doubleq-table-det.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# dqlearning_agent.q_table = './doubleq-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "step = int(num_episodes/50)\n",
    "plt.plot(range(0, num_episodes, step), tr_dq_history[::step], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_dq_history = dqtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "plt.plot(range(0, num_eval_episodes, 25), ev_dq_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of each states action-value estimates\n",
    "mean_q_table =  dqlearning_agent.q_table.mean(axis=0)\n",
    "learned_policy = [denv._action_to_direction[mean_q_table[i].argmax()]['name']\n",
    "                  for i in range(mean_q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((denv.size, denv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "obs, info = denv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(denv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = dqlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = denv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(denv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "denv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "step = int(num_episodes/50)\n",
    "plt.plot(range(0, num_episodes, step), tr_dq_history[::step], label='Double Q-Learning', linewidth=2)\n",
    "plt.plot(range(0, num_episodes, step), tr_q_history[::step], label='Q-Learning', linewidth=2)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Cumulative Reward per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Stocastic Grid Env\n",
    "\n",
    "### Rules:\n",
    "- Landing on a square causes the reward on that square to be consumed. i.e. landing on that square again will yield no reward (0 reward).\n",
    "- Whenever you perform an action there is a 2/3rd chance that the action is as you expect it to be, a 2/9th chance that it is the double of what you want it to be and a 1/9th chance of it being a mirror action.\n",
    "- The reward (and negative reward) for each state is distributed in a reciprocal fashion. i.e. 1/9th the reward when coming through the expected action, 2/9th reward when coming via the opposite action and 2/3rd reward when coming via a mirror action. \n",
    "\n",
    "For instance on a 5x5 grid where the reward on each square is 1 (for simplicity):\n",
    "\n",
    "If `s = (2,3)` and `a = RIGHT`:\n",
    "-   2/3rd chance `s' = (2,4)` and `r = 1/9`\n",
    "-   2/9th chance `s' = (2,5)` and `r = 2/9`\n",
    "-   2/3rd chance `s' = (3,2)` and `r = 2/3`\n",
    "\n",
    "Exception: Landing on the `goal` square will always yield a maximal reward (of 1 for the above example) regardless of how you get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocasticGridEnvironment(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "        \"background_img\": \"./assets/background.jpg\",\n",
    "        \"goal_img\": \"./assets/goal.png\",\n",
    "        \"agent_img\": \"./assets/agent.png\",\n",
    "        \"reward_img\": \"./assets/reward.png\",\n",
    "        \"neg_reward_img\": \"./assets/neg_reward.png\" \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        max_time_steps:int = 20,\n",
    "        render_mode=None,\n",
    "        seed=88\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                \"goal\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                # Randomly distribute rewards between -.5 to .5\n",
    "                # (have an initial upperbound of 1. as that is the goal reward, then set the\n",
    "                # non goal rewards > .5 as .5)\n",
    "                \"reward\": spaces.Box(\n",
    "                    -1., 1.,\n",
    "                    shape=(size**2, ),\n",
    "                    seed=self.np_random,\n",
    "                    dtype=np.float16\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_mode_space = spaces.Discrete(3)\n",
    "        self.max_timesteps = max_time_steps\n",
    "\n",
    "        self._base_states = np.zeros((size, size), dtype=np.float16)\n",
    "\n",
    "        # Add 2 negative rewards\n",
    "        for _ in range(2):\n",
    "            neg_reward_loc = self.observation_space['agent'].sample()\n",
    "            while self._base_states[tuple(neg_reward_loc)] != 0:\n",
    "                neg_reward_loc = self.observation_space['agent'].sample()\n",
    "\n",
    "            self._base_states[tuple(neg_reward_loc)] = -.5\n",
    "\n",
    "        # Add 4 positive rewards\n",
    "        for _ in range(4):\n",
    "            pos_reward_loc = self.observation_space['agent'].sample()\n",
    "            while self._base_states[tuple(pos_reward_loc)] != 0:\n",
    "                pos_reward_loc = self.observation_space['agent'].sample()\n",
    "\n",
    "            self._base_states[tuple(pos_reward_loc)] = .5\n",
    "\n",
    "        \"\"\"The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: {'move': np.array([1, 0]), 'name': 'down'},\n",
    "            1: {'move': np.array([-1, 0]), 'name': 'up'},\n",
    "            2: {'move': np.array([0, 1]), 'name': 'right'},\n",
    "            3: {'move': np.array([0, -1]), 'name': 'left'},\n",
    "        }\n",
    "        self._action_mode_mapping = {\n",
    "            0: {'p': 2/3, 'name': 'as-is', 'score_mul': 1/9},\n",
    "            1: {'p': 2/9, 'name': 'double', 'score_mul': 2/9},\n",
    "            2: {'p': 1/9, 'name': 'mirror', 'score_mul': 2/3}\n",
    "        }\n",
    "\n",
    "        self.timestep = 0\n",
    "        self.states = self._base_states.copy()\n",
    "        # We will sample the goals's location randomly until it does not coincide\n",
    "        # with the reward locations.\n",
    "        self._goal_pos = self.observation_space['goal'].sample()\n",
    "        while self.states[tuple(self._goal_pos)] != 0:\n",
    "            self._goal_pos = self.observation_space['goal'].sample()\n",
    "\n",
    "        self.states[tuple(self._goal_pos)] = self.observation_space['reward'].high[0]\n",
    "\n",
    "        # We will sample the agent's location randomly until it does not coincide\n",
    "        # with the goal and reward locations.\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while self.states[tuple(self._agent_pos)] != 0:\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        # Check for render mode legality\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.window_size = 744  # The size of the PyGame window\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # For compliance\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset env\n",
    "        self.timestep = 0\n",
    "        self.states = self._base_states.copy()\n",
    "        self.states[tuple(self._goal_pos)] = self.observation_space['reward'].high[0]\n",
    "\n",
    "        # Agent start pos changes everytime\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while self.states[tuple(self._agent_pos)] != 0:\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Randomly select the action mode\n",
    "        action_mode = self.np_random.choice(\n",
    "            range(self.action_mode_space.n),\n",
    "            p=[self._action_mode_mapping[i]['p']\n",
    "               for i in range(self.action_mode_space.n)]\n",
    "        )\n",
    "\n",
    "        if self._action_mode_mapping[action_mode]['name'] in ('double', 'as-is'):\n",
    "            direction = self._action_to_direction[action]['move'].copy()\n",
    "            if self._action_mode_mapping[action_mode]['name'] == 'double':\n",
    "                direction *= 2\n",
    "\n",
    "            # Restricting the agent to the grid.\n",
    "            new_pos = np.clip(\n",
    "                self._agent_pos + direction,\n",
    "                0, self.size - 1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            new_pos = self._agent_pos[::-1]\n",
    "\n",
    "        # Give negative reward for actions that keep you in the same\n",
    "        # place (boundary actions)\n",
    "        if np.array_equal(self._agent_pos, new_pos):\n",
    "            reward = self.observation_space['reward'].low[0]\n",
    "        else: reward = self.states[tuple(new_pos)]\n",
    "\n",
    "        self._agent_pos = new_pos\n",
    "\n",
    "        # Record the action taken\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        # Check if terminated\n",
    "        if np.array_equal(self._agent_pos, self._goal_pos):\n",
    "            reward = self.observation_space['reward'].high[0]\n",
    "            terminated = True\n",
    "\n",
    "        else:\n",
    "            reward = np.round(self.states[tuple(self._agent_pos)] * \n",
    "                self._action_mode_mapping[action_mode]['score_mul'], 2)\n",
    "            terminated = False\n",
    "\n",
    "        # Consume reward\n",
    "        self.states[tuple(self._agent_pos)] = 0\n",
    "        \n",
    "        # An episode is done iff the agent has reached the goal\n",
    "        self.timestep += 1\n",
    "\n",
    "        truncated = self.timestep > self.max_timesteps\n",
    "\n",
    "        info = self._get_info()\n",
    "        info.update({\n",
    "            'action': self._action_to_direction[action]['name'],\n",
    "            'action_mode': self._action_mode_mapping[action_mode]['name']\n",
    "        })\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_pos,\n",
    "            \"goal\": self._goal_pos,\n",
    "            \"reward\": self.states.flatten()\n",
    "        }\n",
    "\n",
    "    # City block distance between goal and agent\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_pos - self._goal_pos, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # Background image\n",
    "        bg_img = pygame.image.load(self.metadata['background_img'])\n",
    "        bg_img.set_alpha(188)\n",
    "        bg_img = pygame.transform.scale(bg_img, (self.window_size, self.window_size))\n",
    "        canvas.blit(\n",
    "            bg_img,\n",
    "            [0, 0]\n",
    "        )\n",
    "\n",
    "        # Goal image\n",
    "        goal_img = pygame.image.load(self.metadata['goal_img'])\n",
    "        goal_img = pygame.transform.scale(goal_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            goal_img,\n",
    "            pix_square_size * self._goal_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Agent image\n",
    "        agent_img = pygame.image.load(self.metadata['agent_img'])\n",
    "        agent_img = pygame.transform.scale(agent_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            agent_img,\n",
    "            pix_square_size * self._agent_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Reward image\n",
    "        reward_img = pygame.image.load(self.metadata['reward_img'])\n",
    "        # Negative reward image\n",
    "        neg_reward_img = pygame.image.load(self.metadata['neg_reward_img'])\n",
    "\n",
    "        # Add the reward and neg reward\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                reward = self.states[x, y]\n",
    "                if self.states[x, y] > 0 and self.states[x, y] < 1:\n",
    "                    sreward_img = pygame.transform.scale(\n",
    "                        reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    rew_sz = np.array(sreward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= rew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sreward_img,\n",
    "                        position\n",
    "                    )\n",
    "                elif self.states[x, y] < 0:\n",
    "                    reward *= -1\n",
    "                    sneg_reward_img = pygame.transform.scale(\n",
    "                        neg_reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    nrew_sz = np.array(sneg_reward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= nrew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sneg_reward_img,\n",
    "                        position\n",
    "                    )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent and the env\n",
    "senv = StocasticGridEnvironment(size=6, render_mode='rgb_array')\n",
    "\n",
    "# Check for API Compliance\n",
    "env_checker.check_env(senv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**5\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "qlearning_agent = QLearningAgent(senv)\n",
    "qtrainer = QLearningTrainer(qlearning_agent)\n",
    "tr_q_history = qtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "qlearning_agent.save_pretrained('./q-table-sto.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# qlearning_agent.q_table = './q-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "step = int(num_episodes/50)\n",
    "plt.plot(range(0, num_episodes, step), tr_q_history[::step], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_q_history = qtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "plt.plot(range(0, num_eval_episodes, 25), ev_q_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_policy = [senv._action_to_direction[qlearning_agent.q_table[i].argmax()]['name']\n",
    "                  for i in range(qlearning_agent.q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((senv.size, senv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "obs, info = senv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(senv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = qlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = senv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(senv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "senv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Double Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**5\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "dqlearning_agent = QLearningAgent(senv, doubleq=True)\n",
    "dqtrainer = DoubleQTrainer(dqlearning_agent)\n",
    "tr_dq_history = dqtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "dqlearning_agent.save_pretrained('./doubleq-table-sto.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# dqlearning_agent.q_table = './doubleq-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "step = int(num_episodes/50)\n",
    "plt.plot(range(0, num_episodes, step), tr_dq_history[::step], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_dq_history = dqtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "plt.plot(range(0, num_eval_episodes, 25), ev_dq_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of each states action-value estimates\n",
    "mean_q_table =  dqlearning_agent.q_table.mean(axis=0)\n",
    "learned_policy = [senv._action_to_direction[mean_q_table[i].argmax()]['name']\n",
    "                  for i in range(mean_q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((senv.size, senv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "obs, info = senv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(senv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = dqlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = senv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(senv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "senv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "step = int(num_episodes/50)\n",
    "plt.plot(range(0, num_episodes, step), tr_dq_history[::step], label='Double Q-Learning', linewidth=2)\n",
    "plt.plot(range(0, num_episodes, step), tr_q_history[::step], label='Q-Learning', linewidth=2)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Cumulative Reward per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('expt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
