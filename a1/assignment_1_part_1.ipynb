{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import numpy as np\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Determinisitic Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "        \"goal_img\": \"./goal.png\",\n",
    "        \"agent_img\": \"./agent.png\",\n",
    "        \"reward_img\": \"./reward.png\",\n",
    "        \"neg_reward_img\": \"./neg_reward.png\" \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        max_time_steps:int = 20,\n",
    "        render_mode=None\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), dtype=int),\n",
    "                \"goal\": spaces.Box(0, size - 1, shape=(2, ), dtype=int),\n",
    "            }\n",
    "        )\n",
    "        self.observation_space['agent'].sample()\n",
    "        self.observation_space['goal'].sample()\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = max_time_steps\n",
    "\n",
    "        # Randomly distribute rewards between -0.5 to 0.5\n",
    "        self._base_state = self.np_random.uniform(\n",
    "            -.75, .75, size=(self.size, self.size)\n",
    "        )\n",
    "        np.around(self._base_state, 2, self._base_state)\n",
    "\n",
    "        \"\"\"The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]), # down\n",
    "            1: np.array([-1, 0]), # up\n",
    "            2: np.array([0, 1]), # right\n",
    "            3: np.array([0, -1]), # left\n",
    "        }\n",
    "\n",
    "        self.timestep = 0\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        self._goal_pos = self._agent_pos.copy()\n",
    "\n",
    "        # We will sample the goal's location randomly until it does not coincide\n",
    "        # with the agent's location\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._goal_pos = self.observation_space['goal'].sample()\n",
    "\n",
    "        self.state = self._base_state.copy()\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        # Check for render mode legality\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.window_size = 744  # The size of the PyGame window\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset env\n",
    "        self.state = self._base_state.copy()\n",
    "        # Agent start pos changes everytime\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        direction = self._action_to_direction[action]\n",
    "        \n",
    "        # Restricting the agent to the grid.\n",
    "        self._agent_pos = np.clip(\n",
    "            self._agent_pos + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        reward = self.state[tuple(self._agent_pos)]\n",
    "\n",
    "        # Consume reward\n",
    "        self.state[tuple(self._agent_pos)] = 0        \n",
    "        \n",
    "        # An episode is done iff the agent has reached the goal\n",
    "        self.timestep += 1\n",
    "        terminated = np.array_equal(self._agent_pos, self._goal_pos)\n",
    "\n",
    "        truncated = self.timestep > self.max_timesteps\n",
    "\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\"agent\": self._agent_pos, \"goal\": self._goal_pos}\n",
    "\n",
    "    # City block distance between goal and agent\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_pos - self._goal_pos, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        canvas.fill((255, 255, 255))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # Goal image\n",
    "        goal_img = pygame.image.load(self.metadata['goal_img'])\n",
    "        goal_img = pygame.transform.scale(goal_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            goal_img,\n",
    "            pix_square_size * self._goal_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Agent image\n",
    "        agent_img = pygame.image.load(self.metadata['agent_img'])\n",
    "        agent_img = pygame.transform.scale(agent_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            agent_img,\n",
    "            pix_square_size * self._agent_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Reward image\n",
    "        reward_img = pygame.image.load(self.metadata['reward_img'])\n",
    "        # Negative reward image\n",
    "        neg_reward_img = pygame.image.load(self.metadata['neg_reward_img'])\n",
    "\n",
    "        # Add the reward and neg reward\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                reward = self.state[x, y]\n",
    "                if self.state[x, y] > 0 and self.state[x, y] < 1:\n",
    "                    sreward_img = pygame.transform.scale(\n",
    "                        reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    rew_sz = np.array(sreward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= rew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sreward_img,\n",
    "                        position\n",
    "                    )\n",
    "                elif self.state[x, y] < 0:\n",
    "                    reward *= -1\n",
    "                    sneg_reward_img = pygame.transform.scale(\n",
    "                        neg_reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    nrew_sz = np.array(sneg_reward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= nrew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sneg_reward_img,\n",
    "                        position\n",
    "                    )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=3,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                0,\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=3,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "  def __init__(self, env):\n",
    "    self.env = env\n",
    "    self.observation_space = env.observation_space\n",
    "    self.action_space = env.action_space\n",
    "\n",
    "  def step(self, obs):\n",
    "    return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'agent': array([1, 5]), 'goal': array([4, 2])}\n",
      "{'agent': array([2, 5]), 'goal': array([4, 2])} -0.38\n",
      "{'agent': array([2, 4]), 'goal': array([4, 2])} -0.08\n",
      "{'agent': array([2, 3]), 'goal': array([4, 2])} -0.67\n",
      "{'agent': array([1, 3]), 'goal': array([4, 2])} -0.41\n",
      "{'agent': array([0, 3]), 'goal': array([4, 2])} -0.44\n",
      "{'agent': array([0, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([1, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([2, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([1, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([0, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([0, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([0, 2]), 'goal': array([4, 2])} 0.33\n",
      "{'agent': array([1, 2]), 'goal': array([4, 2])} 0.49\n",
      "{'agent': array([1, 1]), 'goal': array([4, 2])} -0.57\n",
      "{'agent': array([1, 2]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([2, 2]), 'goal': array([4, 2])} -0.52\n",
      "{'agent': array([3, 2]), 'goal': array([4, 2])} -0.61\n",
      "{'agent': array([2, 2]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([2, 3]), 'goal': array([4, 2])} 0.0\n",
      "{'agent': array([3, 3]), 'goal': array([4, 2])} 0.35\n",
      "{'agent': array([2, 3]), 'goal': array([4, 2])} 0.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "env = GridEnvironment(size=6, render_mode='human')\n",
    "agent = RandomAgent(env)\n",
    "\n",
    "obs, info = env.reset()\n",
    "print(obs)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "env.render()\n",
    "while not (terminated or truncated):\n",
    "  action = agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "  env.render()\n",
    "  print(obs, reward)\n",
    "  time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('expt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
