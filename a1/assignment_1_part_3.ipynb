{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d61ffe9e",
   "metadata": {},
   "source": [
    "# <center>CSE 4/546: Reinforcement Learning</center>\n",
    "## <center>Prof. Alina Vereshchaka</center>\n",
    "### <center>Assignment 1</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1cf939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import gymnasium\n",
    "from gymnasium import spaces\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from typing import Any, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db09cc",
   "metadata": {},
   "source": [
    "### Stock Trading Environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f827591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Stock Trading Environment.\n",
    "\"\"\"DON'T MAKE ANY CHANGES TO THE ENVIRONMENT.\"\"\"\n",
    "\n",
    "\n",
    "class StockTradingEnvironment(gymnasium.Env):\n",
    "    \"\"\"This class implements the Stock Trading environment.\"\"\"\n",
    "\n",
    "    def __init__(self, file_path, train=True, number_of_days_to_consider=10):\n",
    "        \"\"\"This method initializes the environment.\n",
    "\n",
    "        :param file_path: - Path of the CSV file containing the historical stock data.\n",
    "        :param train: - Boolean indicating whether the goal is to train or test the performance of the agent.\n",
    "        :param number_of_days_to_consider = Integer representing the number of days the for which the agent\n",
    "                considers the trend in stock price to make a decision.\"\"\"\n",
    "\n",
    "        self.file_path = file_path\n",
    "        self.stock_data = pd.read_csv(self.file_path)\n",
    "        self.train = train\n",
    "\n",
    "        # Splitting the data into train and test datasets.\n",
    "        self.training_stock_data = self.stock_data.iloc[:int(0.8 * len(self.stock_data))]\n",
    "        self.testing_stock_data = self.stock_data.iloc[int(0.8 * len(self.stock_data)):].reset_index()\n",
    "\n",
    "        self.observation_space = spaces.Discrete(4)\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
    "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
    "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
    "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
    "        # This defines the agent's total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # List to store the total account value over training or evaluation.\n",
    "        self.total_account_value_list = []\n",
    "        # This defines the number of days for which the agent considers the data before taking an action.\n",
    "        self.number_of_days_to_consider = number_of_days_to_consider\n",
    "        # The maximum timesteps the agent will take before the episode ends.\n",
    "        if self.train:\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "        else:\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "        # Initializing the number of steps taken to 0.\n",
    "        self.timestep = 0\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"This method resets the environment and returns the observation.\n",
    "\n",
    "        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the\n",
    "                                agent can receive. The observation depends upon whether the price increased on average\n",
    "                                in the number of days the agent considers, and whether the agent already has the stock\n",
    "                                or not.\n",
    "\n",
    "                 info: - info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        self.investment_capital = 100000  # This defines the investment capital that the agent starts with.\n",
    "        self.number_of_shares = 0  # This defines number of shares currently held by the agent.\n",
    "        self.stock_value = 0  # This defines the value of the stock currently held by the agent.\n",
    "        self.book_value = 0  # This defines the total value for which the agent bought the shares.\n",
    "        # This defines the agent's total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # List to store the total account value over training or evaluation.\n",
    "        self.total_account_value_list = []\n",
    "        # Initializing the number of steps taken to 0.\n",
    "        self.timestep = 0\n",
    "\n",
    "        # Getting the observation vector.\n",
    "        if self.train:\n",
    "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
    "            # subtracted from the  length of the training stock data.\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            stock_held = False\n",
    "\n",
    "            # Observation vector that will be passed to the agent.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        else:\n",
    "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
    "            # number of days considered subtracted from the  length of the testing stock data.\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            stock_held = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        if np.array_equal(observation, [True, False]):\n",
    "            observation = 0\n",
    "        if np.array_equal(observation, [True, True]):\n",
    "            observation = 1\n",
    "        if np.array_equal(observation, [False, False]):\n",
    "            observation = 2\n",
    "        if np.array_equal(observation, [False, True]):\n",
    "            observation = 3\n",
    "\n",
    "        info = None\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"This method implements what happens when the agent takes the action to Buy/Sell/Hold.\n",
    "\n",
    "        :param action: - Integer in the range 0 to 2 inclusive.\n",
    "\n",
    "        :returns observation: - Integer in the range of 0 to 3 representing the four possible observations that the\n",
    "                                agent can receive. The observation depends upon whether the price increased on average\n",
    "                                in the number of days the agent considers, and whether the agent already has the stock\n",
    "                                or not.\n",
    "                 reward: - Integer/Float value that's used to measure the performance of the agent.\n",
    "                 terminated: - Boolean describing whether the episode has terminated.\n",
    "                 truncated: - Boolean describing whether a truncation condition outside the scope of the MDP is satisfied.\n",
    "                 info: - A dictionary that can be used to provide additional implementation information.\"\"\"\n",
    "\n",
    "        # We give the agent a penalty for taking actions such as buying a stock when the agent doesn't have the\n",
    "        # investment capital and selling a stock when the agent doesn't have any shares.\n",
    "        penalty = 0\n",
    "\n",
    "        if self.train:\n",
    "            if action == 0:  # Buy\n",
    "                if self.number_of_shares > 0:\n",
    "                    penalty = -10\n",
    "                # Determining the number of shares the agent can buy.\n",
    "                number_of_shares_to_buy = math.floor(self.investment_capital / self.training_stock_data[\n",
    "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
    "                # Adding to the number of shares the agent has.\n",
    "                self.number_of_shares += number_of_shares_to_buy\n",
    "\n",
    "                # Computing the stock value, book value, investment capital and reward.\n",
    "                if number_of_shares_to_buy > 0:\n",
    "                    self.stock_value +=\\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.book_value += \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider]\\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.investment_capital -= \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "\n",
    "                    reward = 1 + penalty\n",
    "\n",
    "                else:\n",
    "                    # Computing the stock value and reward.\n",
    "                    self.stock_value = \\\n",
    "                        self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * self.number_of_shares\n",
    "                    reward = -10\n",
    "\n",
    "            if action == 1:  # Sell\n",
    "                # Computing the investment capital, sell value and reward.\n",
    "                self.investment_capital += \\\n",
    "                    self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                    * self.number_of_shares\n",
    "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                             * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -10\n",
    "\n",
    "                self.number_of_shares = 0\n",
    "                self.stock_value = 0\n",
    "                self.book_value = 0\n",
    "\n",
    "            if action == 2:  # Hold\n",
    "                # Computing the stock value and reward.\n",
    "                self.stock_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                                   * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "        else:\n",
    "            if action == 0:  # Buy\n",
    "                if self.number_of_shares > 0:\n",
    "                    penalty = -10\n",
    "                # Determining the number of shares the agent can buy.\n",
    "                number_of_shares_to_buy = math.floor(self.investment_capital / self.testing_stock_data[\n",
    "                    'Open'][self.timestep + self.number_of_days_to_consider])\n",
    "                # Adding to the number of shares the agent has.\n",
    "                self.number_of_shares += number_of_shares_to_buy\n",
    "\n",
    "                # Computing the stock value, book value, investment capital and reward.\n",
    "                if number_of_shares_to_buy > 0:\n",
    "                    self.stock_value += \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.book_value += \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "                    self.investment_capital -= \\\n",
    "                        self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                        * number_of_shares_to_buy\n",
    "\n",
    "                    reward = 1 + penalty\n",
    "\n",
    "                else:\n",
    "                    # Computing the stock value and reward.\n",
    "                    self.stock_value = self.training_stock_data['Open'][\n",
    "                                           self.timestep + self.number_of_days_to_consider] * self.number_of_shares\n",
    "                    reward = -10\n",
    "\n",
    "            if action == 1:  # Sell\n",
    "                # Computing the investment capital, sell value and reward.\n",
    "                self.investment_capital += \\\n",
    "                    self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                    * self.number_of_shares\n",
    "                sell_value = self.training_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                             * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (sell_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -10\n",
    "\n",
    "                self.number_of_shares = 0\n",
    "                self.stock_value = 0\n",
    "                self.book_value = 0\n",
    "\n",
    "            if action == 2:  # Hold\n",
    "                # Computing the stock value and reward.\n",
    "                self.stock_value = self.testing_stock_data['Open'][self.timestep + self.number_of_days_to_consider] \\\n",
    "                                   * self.number_of_shares\n",
    "\n",
    "                if self.book_value > 0:\n",
    "                    reward = (self.stock_value - self.book_value) / self.book_value * 100\n",
    "                else:\n",
    "                    reward = -1\n",
    "\n",
    "        # Determining if the agent currently has shares of the stock or not.\n",
    "        if self.number_of_shares > 0:\n",
    "            stock_held = True\n",
    "        else:\n",
    "            stock_held = False\n",
    "\n",
    "        # Getting the observation vector.\n",
    "        if self.train:\n",
    "            # If the task is to train the agent the maximum timesteps will be equal to the number of days considered\n",
    "            # subtracted from the  length of the training stock data.\n",
    "            self.max_timesteps = len(self.training_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.training_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.training_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        else:\n",
    "            # If the task is to evaluate the trained agent's performance the maximum timesteps will be equal to the\n",
    "            # number of days considered subtracted from the  length of the testing stock data.\n",
    "            self.max_timesteps = len(self.testing_stock_data) - self.number_of_days_to_consider\n",
    "\n",
    "            # Calculating whether the price increased or decreased/remained the same on the majority of days the agent\n",
    "            # considers.\n",
    "            price_increase_list = []\n",
    "            for i in range(self.number_of_days_to_consider):\n",
    "                if self.testing_stock_data['Close'][self.timestep + 1 + i] \\\n",
    "                        - self.testing_stock_data['Close'][self.timestep + i] > 0:\n",
    "                    price_increase_list.append(1)\n",
    "                else:\n",
    "                    price_increase_list.append(0)\n",
    "\n",
    "            if (np.sum(price_increase_list) / self.number_of_days_to_consider) >= 0.5:\n",
    "                price_increase = True\n",
    "            else:\n",
    "                price_increase = False\n",
    "\n",
    "            # Observation vector.\n",
    "            observation = [price_increase, stock_held]\n",
    "\n",
    "        self.timestep += 1  # Increasing the number of steps taken by the agent by 1.\n",
    "\n",
    "        if np.array_equal(observation, [True, False]):\n",
    "            observation = 0\n",
    "        if np.array_equal(observation, [True, True]):\n",
    "            observation = 1\n",
    "        if np.array_equal(observation, [False, False]):\n",
    "            observation = 2\n",
    "        if np.array_equal(observation, [False, True]):\n",
    "            observation = 3\n",
    "\n",
    "        # Computing the total account value.\n",
    "        self.total_account_value = self.investment_capital + self.stock_value\n",
    "        # Appending the total account value of the list to plot the graph.\n",
    "        self.total_account_value_list.append(self.total_account_value)\n",
    "\n",
    "        # The episode terminates when the maximum timesteps have been reached.\n",
    "        terminated = True if (self.timestep >= self.max_timesteps) \\\n",
    "            else False\n",
    "        truncated = False\n",
    "        info = {}\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        \"\"\"This method renders the agent's total account value over time.\n",
    "\n",
    "        :param mode: 'human' renders to the current display or terminal and returns nothing.\"\"\"\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.plot(self.total_account_value_list, color='lightseagreen', linewidth=7)\n",
    "        plt.xlabel('Days', fontsize=32)\n",
    "        plt.ylabel('Total Account Value', fontsize=32)\n",
    "        plt.title('Total Account Value over Time', fontsize=38)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca06c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: You can adjust the parameter 'number_of_days_to_consider'\n",
    "\n",
    "stock_trading_environment = StockTradingEnvironment('./NVDA.csv', number_of_days_to_consider=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Define the Q Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff10c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "  def __init__(self, env: gymnasium.Env, doubleq: bool = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      env (Env): The environment the agent should act in.\n",
    "      doubleq (bool): Whether to use double Q-Learning.\n",
    "    \"\"\"\n",
    "    # Intialize Q-Table\n",
    "    self.env = env\n",
    "    self.doubleq = doubleq\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # To allow usage of this agent with the stock trading env\n",
    "    if isinstance(env.observation_space, spaces.Dict):\n",
    "      num_states = (env.observation_space['agent'].high+1).prod()\n",
    "    else:\n",
    "      num_states = env.action_space.n\n",
    "\n",
    "    # Intialize 2 Q tables if `doubleq` is True\n",
    "    if doubleq: q_table_shape = (2, num_states, num_actions)\n",
    "    else: q_table_shape = (num_states, num_actions)\n",
    "\n",
    "    self._q_table = np.zeros((\n",
    "      q_table_shape\n",
    "    ))\n",
    "\n",
    "  @property\n",
    "  def q_table(self) -> np.ndarray:\n",
    "    return self._q_table\n",
    "\n",
    "  @q_table.setter\n",
    "  def q_table(self, path_to_q_table: str):\n",
    "    with open(path_to_q_table, 'rb') as f:\n",
    "      q_table = pickle.load(f)\n",
    "\n",
    "    # Check if the table is compatible\n",
    "    if self.q_table.shape != q_table.shape:\n",
    "      raise ValueError(\"Trying to load incompatible Q-Table.\")\n",
    "\n",
    "    self._q_table = q_table\n",
    "\n",
    "  def greedy_policy(self, state):\n",
    "    if self.doubleq:\n",
    "      # Choose the best action after taking the mean value of\n",
    "      # the actions accross both tables.\n",
    "      return self.q_table.mean(axis=0)[state].argmax()\n",
    "\n",
    "    else: return self.q_table[state].argmax()\n",
    "\n",
    "  def ep_greedy_policy(self, state, epsilon):\n",
    "    random_num = self.env.np_random.uniform(0, 1)\n",
    "\n",
    "    if random_num > epsilon:\n",
    "      return self.greedy_policy(state)\n",
    "\n",
    "    else:\n",
    "      return self.env.action_space.sample()\n",
    "\n",
    "  def _get_state_idx(self, observation: Any) -> int:\n",
    "    \"\"\"Helper func to allow the same Agent to be used with the stock\n",
    "    prediction env.\n",
    "    \"\"\"\n",
    "    if isinstance(observation, dict): state = observation['agent']\n",
    "    else: state = observation\n",
    "\n",
    "    # Convert 2-d state index to flat index\n",
    "    # flat_idx = curr_row*num_cols+curr_col\n",
    "    if isinstance(state, np.ndarray): \n",
    "      return state[0] * self.env.size + state[1]\n",
    "    else:\n",
    "      return state  \n",
    "\n",
    "  def step(self, observation):\n",
    "    state = self._get_state_idx(observation)\n",
    "    return self.greedy_policy(state)\n",
    "\n",
    "  def save_pretrained(self, path_to_q_table: str):\n",
    "    with open(path_to_q_table, 'wb') as f:\n",
    "      pickle.dump(self.q_table, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4bcefa58",
   "metadata": {},
   "source": [
    "## Define the Q-Learning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fac093",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTrainer:\n",
    "    def __init__(self, agent: QLearningAgent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_episodes: int = 10**5,\n",
    "        lr: float = .4,\n",
    "        gamma: float = .9,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = 1.,\n",
    "        decay_rate: float = .0008\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 10**5.\n",
    "        lr (float, optional): The learning rate. Defaults to .4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to .0008.\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            state = self.agent._get_state_idx(obs)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                action = self.agent.ep_greedy_policy(state, epsilon)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "                new_state = self.agent._get_state_idx(obs)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the Q table\n",
    "                next_state_qvalue = self.agent.q_table[new_state].max()\n",
    "\n",
    "                self.agent.q_table[state, action] += lr * \\\n",
    "                (reward+gamma*next_state_qvalue-self.agent.q_table[state, action])\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n",
    "\n",
    "    def evaluate(self, num_episodes: int = 10**2) -> List[float]:\n",
    "        \"\"\"Evaluate the agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): Defaults to 10**2.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                action = self.agent.step(obs)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
