{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intializing stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'pwd' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'which' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.1.2 (SDL 2.0.18, Python 3.8.13)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import pygame\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from typing import List, Any\n",
    "\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces\n",
    "from gymnasium.utils import env_checker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display set up\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "os.environ[\"SDL_VIDEODRIVER\"] = \"dummy\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "  def __init__(self, env: Env, doubleq: bool = False):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      env (Env): The environment the agent should act in.\n",
    "      doubleq (bool): Whether to use double Q-Learning.\n",
    "    \"\"\"\n",
    "    # Intialize Q-Table\n",
    "    self.env = env\n",
    "    self.doubleq = doubleq\n",
    "\n",
    "    num_actions = env.action_space.n\n",
    "\n",
    "    # To allow usage of this agent with the stock trading env\n",
    "    if isinstance(env.observation_space, spaces.Dict):\n",
    "      num_states = (env.observation_space['agent'].high+1).prod()\n",
    "    else:\n",
    "      num_states = env.action_space.n\n",
    "\n",
    "    # Intialize 2 Q tables if `doubleq` is True\n",
    "    if doubleq: q_table_shape = (2, num_states, num_actions)\n",
    "    else: q_table_shape = (num_states, num_actions)\n",
    "\n",
    "    self._q_table = np.zeros((\n",
    "      q_table_shape\n",
    "    ))\n",
    "\n",
    "  @property\n",
    "  def q_table(self) -> np.ndarray:\n",
    "    return self._q_table\n",
    "\n",
    "  @q_table.setter\n",
    "  def q_table(self, path_to_q_table: str):\n",
    "    with open(path_to_q_table, 'rb') as f:\n",
    "      q_table = pickle.load(f)\n",
    "\n",
    "    # Check if the table is compatible\n",
    "    if self.q_table.shape != q_table.shape:\n",
    "      raise ValueError(\"Trying to load incompatible Q-Table.\")\n",
    "\n",
    "    self._q_table = q_table\n",
    "\n",
    "  def greedy_policy(self, state):\n",
    "    if self.doubleq:\n",
    "      # Choose the best action after taking the mean value of\n",
    "      # the actions accross both tables.\n",
    "      return self.q_table.mean(axis=0)[state].argmax()\n",
    "\n",
    "    else: return self.q_table[state].argmax()\n",
    "\n",
    "  def ep_greedy_policy(self, state, epsilon):\n",
    "    random_num = self.env.np_random.uniform(0, 1)\n",
    "\n",
    "    if random_num > epsilon:\n",
    "      return self.greedy_policy(state)\n",
    "\n",
    "    else:\n",
    "      return self.env.action_space.sample()\n",
    "\n",
    "  def _get_state_idx(self, observation: Any) -> int:\n",
    "    \"\"\"Helper func to allow the same Agent to be used with the stock\n",
    "    prediction env.\n",
    "    \"\"\"\n",
    "    if isinstance(observation, dict): state = observation['agent']\n",
    "    else: state = observation\n",
    "\n",
    "    # Convert 2-d state index to flat index\n",
    "    # flat_idx = curr_row*num_cols+curr_col\n",
    "    if isinstance(state, np.ndarray): \n",
    "      return state[0] * self.env.size + state[1]\n",
    "    else:\n",
    "      return state  \n",
    "\n",
    "  def step(self, observation):\n",
    "    state = self._get_state_idx(observation)\n",
    "    return self.greedy_policy(state)\n",
    "\n",
    "  def save_pretrained(self, path_to_q_table: str):\n",
    "    with open(path_to_q_table, 'wb') as f:\n",
    "      pickle.dump(self.q_table, f)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Trainers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTrainer:\n",
    "    def __init__(self, agent: QLearningAgent):\n",
    "        self.agent = agent\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_episodes: int = 10**5,\n",
    "        lr: float = .4,\n",
    "        gamma: float = .9,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = 1.,\n",
    "        decay_rate: float = .0008\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 10**5.\n",
    "        lr (float, optional): The learning rate. Defaults to .4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to .0008.\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                state = self.agent._get_state_idx(obs)\n",
    "                action = self.agent.ep_greedy_policy(state, epsilon)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "                new_state = self.agent._get_state_idx(obs)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                # Update the Q table\n",
    "                next_state_qvalue = self.agent.q_table[new_state].max()\n",
    "\n",
    "                self.agent.q_table[state, action] += lr * \\\n",
    "                (reward+gamma*next_state_qvalue-self.agent.q_table[state, action])\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n",
    "\n",
    "    def evaluate(self, num_episodes: int = 10**2) -> List[float]:\n",
    "        \"\"\"Evaluate the agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): Defaults to 10**5.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for _ in tqdm(range(num_episodes)):\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                state = self.agent._get_state_idx(obs)\n",
    "                action = self.agent.greedy_policy(state)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "                new_state = self.agent._get_state_idx(obs)\n",
    "\n",
    "                total_reward += reward\n",
    "                state = new_state\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q-Learning Trainer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQTrainer(QLearningTrainer):\n",
    "    def __init__(self, agent: QLearningAgent):\n",
    "        assert agent.doubleq, \"The agent passed isn't configured for Double Q \"\\\n",
    "            \"learning\"\n",
    "        super().__init__(agent)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        num_episodes: int = 10**5,\n",
    "        lr: float = .4,\n",
    "        gamma: float = .9,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = 1.,\n",
    "        decay_rate: float = .0008\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 10**5.\n",
    "        lr (float, optional): The learning rate. Defaults to .4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to .0008.\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        reward_history: List[float] = list()\n",
    "        for episode in tqdm(range(num_episodes)):\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            obs, _ = self.agent.env.reset(seed=88)\n",
    "            terminated, truncated = False, False\n",
    "\n",
    "            # Run the episode\n",
    "            while not (terminated or truncated):\n",
    "                state = self.agent._get_state_idx(obs)\n",
    "                action = self.agent.ep_greedy_policy(state, epsilon)\n",
    "\n",
    "                obs, reward, terminated, truncated, _ = self.agent.env.step(action)\n",
    "                new_state = self.agent._get_state_idx(obs)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                # Choose the table to update\n",
    "                upd_idx = self.agent.env.np_random.choice([0, 1])\n",
    "                eval_idx = 1-upd_idx\n",
    "\n",
    "                # Update the Q table\n",
    "                next_state_qvalue = self.agent.q_table[\n",
    "                    eval_idx,\n",
    "                    new_state,\n",
    "                    self.agent.q_table[upd_idx, new_state].argmax()\n",
    "                ]\n",
    "\n",
    "                self.agent.q_table[upd_idx, state, action] += lr * \\\n",
    "                (reward+gamma*next_state_qvalue-self.agent.q_table[upd_idx, state, action])\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            reward_history.append(total_reward)\n",
    "\n",
    "        return reward_history\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Determinisitic Grid Env\n",
    "\n",
    "### Rules:\n",
    "- Each square has a randomly assigned reward between -0.5 to 0.5. The goal has a reward of 1.0\n",
    "- The size of the cactus/lemonade is indicative of the reward.\n",
    "- Landing on a square causes the reward on that square to be consumed. i.e. landing on that square again will yield no reward (0 reward)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeterministicGridEnvironment(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "        \"background_img\": \"./assets/background.jpg\",\n",
    "        \"goal_img\": \"./assets/goal.png\",\n",
    "        \"agent_img\": \"./assets/agent.png\",\n",
    "        \"reward_img\": \"./assets/reward.png\",\n",
    "        \"neg_reward_img\": \"./assets/neg_reward.png\" \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        max_time_steps:int = 20,\n",
    "        render_mode=None,\n",
    "        seed=34\n",
    "    ):\n",
    "        # Seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                \"goal\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                # Randomly distribute rewards between -.5 to .5\n",
    "                # (have an initial upperbound of 1. as that is the goal reward, then set the\n",
    "                # non goal rewards > .5 as .5)\n",
    "                \"reward\": spaces.Box(\n",
    "                    -1., 1.,\n",
    "                    shape=(size**2, ),\n",
    "                    seed=self.np_random,\n",
    "                    dtype=np.float16\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        self.observation_space['agent'].sample()\n",
    "        self.observation_space['goal'].sample()\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_mode_space = spaces.Discrete(3)\n",
    "        self.max_timesteps = max_time_steps\n",
    "\n",
    "        self._base_state = self.observation_space['reward'].sample(\n",
    "            ).reshape((size, size))\n",
    "        self._base_state = np.clip(self._base_state, -.5, .5)\n",
    "        np.around(self._base_state, 2, self._base_state)\n",
    "\n",
    "        \"\"\"The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: {'move': np.array([1, 0]), 'name': 'down'},\n",
    "            1: {'move': np.array([-1, 0]), 'name': 'up'},\n",
    "            2: {'move': np.array([0, 1]), 'name': 'right'},\n",
    "            3: {'move': np.array([0, -1]), 'name': 'left'},\n",
    "        }\n",
    "\n",
    "        self.timestep = 0\n",
    "        self._goal_pos = self.observation_space['goal'].sample()\n",
    "\n",
    "        # We will sample the agent's location randomly until it does not coincide\n",
    "        # with the agent's location\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        self.state = self._base_state.copy()\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        # Check for render mode legality\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.window_size = 744  # The size of the PyGame window\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # For compliance\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset env\n",
    "        self.timestep = 0\n",
    "        self.state = self._base_state.copy()\n",
    "\n",
    "        # Agent start pos changes everytime\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        direction = self._action_to_direction[action]['move']\n",
    "        \n",
    "        # Restricting the agent to the grid.\n",
    "        self._agent_pos = np.clip(\n",
    "            self._agent_pos + direction,\n",
    "            0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # Record the action taken\n",
    "        observation = self._get_obs()\n",
    "        \n",
    "        reward = self.state[tuple(self._agent_pos)]\n",
    "\n",
    "        # Consume reward\n",
    "        self.state[tuple(self._agent_pos)] = 0        \n",
    "        \n",
    "        # An episode is done iff the agent has reached the goal\n",
    "        self.timestep += 1\n",
    "        terminated = np.array_equal(self._agent_pos, self._goal_pos)\n",
    "\n",
    "        truncated = self.timestep > self.max_timesteps\n",
    "\n",
    "        info = self._get_info()\n",
    "        info.update({\n",
    "            'action': self._action_to_direction[action]['name'] \n",
    "        })\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_pos,\n",
    "            \"goal\": self._goal_pos,\n",
    "            \"reward\": self.state.flatten()\n",
    "        }\n",
    "\n",
    "    # City block distance between goal and agent\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_pos - self._goal_pos, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # Background image\n",
    "        bg_img = pygame.image.load(self.metadata['background_img'])\n",
    "        bg_img.set_alpha(188)\n",
    "        bg_img = pygame.transform.scale(bg_img, (self.window_size, self.window_size))\n",
    "        canvas.blit(\n",
    "            bg_img,\n",
    "            [0, 0]\n",
    "        )\n",
    "\n",
    "        # Goal image\n",
    "        goal_img = pygame.image.load(self.metadata['goal_img'])\n",
    "        goal_img = pygame.transform.scale(goal_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            goal_img,\n",
    "            pix_square_size * self._goal_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Agent image\n",
    "        agent_img = pygame.image.load(self.metadata['agent_img'])\n",
    "        agent_img = pygame.transform.scale(agent_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            agent_img,\n",
    "            pix_square_size * self._agent_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Reward image\n",
    "        reward_img = pygame.image.load(self.metadata['reward_img'])\n",
    "        # Negative reward image\n",
    "        neg_reward_img = pygame.image.load(self.metadata['neg_reward_img'])\n",
    "\n",
    "        # Add the reward and neg reward\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                reward = self.state[x, y]\n",
    "                if self.state[x, y] > 0 and self.state[x, y] < 1:\n",
    "                    sreward_img = pygame.transform.scale(\n",
    "                        reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    rew_sz = np.array(sreward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= rew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sreward_img,\n",
    "                        position\n",
    "                    )\n",
    "                elif self.state[x, y] < 0:\n",
    "                    reward *= -1\n",
    "                    sneg_reward_img = pygame.transform.scale(\n",
    "                        neg_reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    nrew_sz = np.array(sneg_reward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= nrew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sneg_reward_img,\n",
    "                        position\n",
    "                    )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent and the env\n",
    "denv = DeterministicGridEnvironment(size=6, render_mode='rgb_array')\n",
    "\n",
    "# Check for API Compliance\n",
    "env_checker.check_env(denv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 63243/100000 [00:37<00:21, 1681.28it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\repos\\cse546-rl-assignments\\a1\\assignment_1_part_2_test.ipynb Cell 18\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m qlearning_agent \u001b[39m=\u001b[39m QLearningAgent(denv)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m qtrainer \u001b[39m=\u001b[39m QLearningTrainer(qlearning_agent)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m tr_q_history \u001b[39m=\u001b[39m qtrainer\u001b[39m.\u001b[39;49mtrain()\n",
      "\u001b[1;32mc:\\repos\\cse546-rl-assignments\\a1\\assignment_1_part_2_test.ipynb Cell 18\u001b[0m in \u001b[0;36mQLearningTrainer.train\u001b[1;34m(self, num_episodes, lr, gamma, ep_min, ep_max, decay_rate)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39m_get_state_idx(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39mep_greedy_policy(state, epsilon)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m obs, reward, terminated, truncated, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m new_state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39magent\u001b[39m.\u001b[39m_get_state_idx(obs)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m total_reward \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m reward\n",
      "\u001b[1;32mc:\\repos\\cse546-rl-assignments\\a1\\assignment_1_part_2_test.ipynb Cell 18\u001b[0m in \u001b[0;36mDeterministicGridEnvironment.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_pos \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mclip(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_pos \u001b[39m+\u001b[39m direction,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=111'>112</a>\u001b[0m     \u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m \u001b[39m# Record the action taken\u001b[39;00m\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m observation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_obs()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=117'>118</a>\u001b[0m reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate[\u001b[39mtuple\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_pos)]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=119'>120</a>\u001b[0m \u001b[39m# Consume reward\u001b[39;00m\n",
      "\u001b[1;32mc:\\repos\\cse546-rl-assignments\\a1\\assignment_1_part_2_test.ipynb Cell 18\u001b[0m in \u001b[0;36mDeterministicGridEnvironment._get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=138'>139</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_obs\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=139'>140</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=140'>141</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39magent\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_agent_pos,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=141'>142</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mgoal\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_goal_pos,\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=142'>143</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mreward\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstate\u001b[39m.\u001b[39;49mflatten()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/repos/cse546-rl-assignments/a1/assignment_1_part_2_test.ipynb#Y150sZmlsZQ%3D%3D?line=143'>144</a>\u001b[0m     }\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_episodes = 10**5\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "qlearning_agent = QLearningAgent(denv)\n",
    "qtrainer = QLearningTrainer(qlearning_agent)\n",
    "tr_q_history = qtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "qlearning_agent.save_pretrained('./q-table-det.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# qlearning_agent.q_table = './q-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_episodes, 2500), tr_q_history[::2500], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_q_history = qtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_eval_episodes, 25), ev_q_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_policy = [denv._action_to_direction[qlearning_agent.q_table[i].argmax()]['name']\n",
    "                  for i in range(qlearning_agent.q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((denv.size, denv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "obs, info = denv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(denv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = qlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = denv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(denv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "denv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Double Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**4\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "dqlearning_agent = QLearningAgent(denv, doubleq=True)\n",
    "dqtrainer = DoubleQTrainer(dqlearning_agent)\n",
    "tr_dq_history = dqtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "dqlearning_agent.save_pretrained('./doubleq-table-det.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# dqlearning_agent.q_table = './doubleq-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_episodes, 250), tr_dq_history[::250], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_dq_history = dqtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_eval_episodes, 25), ev_dq_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of each states action-value estimates\n",
    "mean_q_table =  dqlearning_agent.q_table.mean(axis=0)\n",
    "learned_policy = [denv._action_to_direction[mean_q_table[i].argmax()]['name']\n",
    "                  for i in range(mean_q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((denv.size, denv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "obs, info = denv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(denv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = dqlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = denv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(denv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "denv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, 10**4, 250), tr_dq_history[::250], label='Double Q-Learning', linewidth=2)\n",
    "plt.plot(range(0, 10**4, 250), tr_q_history[:10**4:250], label='Q-Learning', linewidth=2)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Cumulative Reward per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Stocastic Grid Env\n",
    "\n",
    "### Rules:\n",
    "- Landing on a square causes the reward on that square to be consumed. i.e. landing on that square again will yield no reward (0 reward).\n",
    "- Whenever you perform an action there is a 2/3rd chance that the action is as you expect it to be, a 2/9th chance that it is the double of what you want it to be and a 1/9th chance of it being a mirror action.\n",
    "- The reward (and negative reward) for each state is distributed in a reciprocal fashion. i.e. 1/9th the reward when coming through the expected action, 2/9th reward when coming via the opposite action and 2/3rd reward when coming via a mirror action. \n",
    "\n",
    "For instance on a 5x5 grid where the reward on each square is 1 (for simplicity):\n",
    "\n",
    "If `s = (2,3)` and `a = RIGHT`:\n",
    "-   2/3rd chance `s' = (2,4)` and `r = 1/9`\n",
    "-   2/9th chance `s' = (2,5)` and `r = 2/9`\n",
    "-   2/3rd chance `s' = (3,2)` and `r = 2/3`\n",
    "\n",
    "Exception: Landing on the `goal` square will always yield a maximal reward (of 1 for the above example) regardless of how you get there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StocasticGridEnvironment(Env):\n",
    "    metadata = {\n",
    "        \"render_modes\": [\"human\", \"rgb_array\"],\n",
    "        \"render_fps\": 4,\n",
    "        \"background_img\": \"./assets/background.jpg\",\n",
    "        \"goal_img\": \"./assets/goal.png\",\n",
    "        \"agent_img\": \"./assets/agent.png\",\n",
    "        \"reward_img\": \"./assets/reward.png\",\n",
    "        \"neg_reward_img\": \"./assets/neg_reward.png\" \n",
    "    }\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        size: int = 4,\n",
    "        max_time_steps:int = 20,\n",
    "        render_mode=None,\n",
    "        seed=34\n",
    "    ):\n",
    "        super().reset(seed=seed)\n",
    "        self.size = size\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {\n",
    "                \"agent\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                \"goal\": spaces.Box(0, size - 1, shape=(2, ), seed=self.np_random, dtype=int),\n",
    "                # Randomly distribute rewards between -.5 to .5\n",
    "                # (have an initial upperbound of 1. as that is the goal reward, then set the\n",
    "                # non goal rewards > .5 as .5)\n",
    "                \"reward\": spaces.Box(\n",
    "                    -1., 1.,\n",
    "                    shape=(size**2, ),\n",
    "                    seed=self.np_random,\n",
    "                    dtype=np.float16\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "        self.observation_space['agent'].sample()\n",
    "        self.observation_space['goal'].sample()\n",
    "\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.action_mode_space = spaces.Discrete(3)\n",
    "        self.max_timesteps = max_time_steps\n",
    "\n",
    "        self._base_state = self.observation_space['reward'].sample(\n",
    "            ).reshape((size, size))\n",
    "        self._base_state = np.clip(self._base_state, -.5, .5)\n",
    "        np.around(self._base_state, 2, self._base_state)\n",
    "\n",
    "        \"\"\"The following dictionary maps abstract actions from `self.action_space` to\n",
    "        the direction we will walk in if that action is taken.\n",
    "        I.e. 0 corresponds to \"right\", 1 to \"up\" etc.\n",
    "        \"\"\"\n",
    "        self._action_to_direction = {\n",
    "            0: {'move': np.array([1, 0]), 'name': 'down'},\n",
    "            1: {'move': np.array([-1, 0]), 'name': 'up'},\n",
    "            2: {'move': np.array([0, 1]), 'name': 'right'},\n",
    "            3: {'move': np.array([0, -1]), 'name': 'left'},\n",
    "        }\n",
    "        self._action_mode_mapping = {\n",
    "            0: {'p': 2/3, 'name': 'as-is', 'score_mul': 1/9},\n",
    "            1: {'p': 2/9, 'name': 'double', 'score_mul': 2/9},\n",
    "            2: {'p': 1/9, 'name': 'mirror', 'score_mul': 2/3}\n",
    "        }\n",
    "\n",
    "        self.timestep = 0\n",
    "        self._goal_pos = self.observation_space['goal'].sample()\n",
    "\n",
    "        # We will sample the agent's location randomly until it does not coincide\n",
    "        # with the agent's location\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        self.state = self._base_state.copy()\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        # Check for render mode legality\n",
    "        assert render_mode is None or render_mode in self.metadata[\"render_modes\"]\n",
    "        self.render_mode = render_mode\n",
    "\n",
    "        self.window = None\n",
    "        self.clock = None\n",
    "        self.window_size = 744  # The size of the PyGame window\n",
    "\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # For compliance\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Reset env\n",
    "        self.timestep = 0\n",
    "        self.state = self._base_state.copy()\n",
    "\n",
    "        # Agent start pos changes everytime\n",
    "        self._agent_pos = self.observation_space['agent'].sample()\n",
    "        while np.array_equal(self._goal_pos, self._agent_pos):\n",
    "            self._agent_pos = self.observation_space['agent'].sample()\n",
    "\n",
    "        self.state[tuple(self._goal_pos)] = 1. # Max Reward\n",
    "        self.state[tuple(self._agent_pos)] = 0. # 0 reward at start\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # Randomly select the action mode\n",
    "        action_mode = self.np_random.choice(\n",
    "            range(self.action_mode_space.n),\n",
    "            p=[self._action_mode_mapping[i]['p']\n",
    "               for i in range(self.action_mode_space.n)]\n",
    "        )\n",
    "\n",
    "        if self._action_mode_mapping[action_mode]['name'] in ('double', 'as-is'):\n",
    "            direction = self._action_to_direction[action]['move'].copy()\n",
    "            if self._action_mode_mapping[action_mode]['name'] == 'double':\n",
    "                direction *= 2\n",
    "\n",
    "            # Restricting the agent to the grid.\n",
    "            self._agent_pos = np.clip(\n",
    "                self._agent_pos + direction,\n",
    "                0, self.size - 1\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            self._agent_pos = self._agent_pos[::-1]\n",
    "\n",
    "        # Record the action taken\n",
    "        observation = self._get_obs()\n",
    "\n",
    "        # Check if terminated\n",
    "        if np.array_equal(self._agent_pos, self._goal_pos):\n",
    "            reward = 1.\n",
    "            terminated = True\n",
    "\n",
    "        else:\n",
    "            reward = np.round(self.state[tuple(self._agent_pos)] * \n",
    "                self._action_mode_mapping[action_mode]['score_mul'], 2)\n",
    "            terminated = False\n",
    "\n",
    "        # Consume reward\n",
    "        self.state[tuple(self._agent_pos)] = 0        \n",
    "        \n",
    "        # An episode is done iff the agent has reached the goal\n",
    "        self.timestep += 1\n",
    "\n",
    "        truncated = self.timestep > self.max_timesteps\n",
    "\n",
    "        info = self._get_info()\n",
    "        info.update({\n",
    "            'action': self._action_to_direction[action]['name'],\n",
    "            'action_mode': self._action_mode_mapping[action_mode]['name']\n",
    "        })\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return {\n",
    "            \"agent\": self._agent_pos,\n",
    "            \"goal\": self._goal_pos,\n",
    "            \"reward\": self.state.flatten()\n",
    "        }\n",
    "\n",
    "    # City block distance between goal and agent\n",
    "    def _get_info(self):\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_pos - self._goal_pos, ord=1\n",
    "            )\n",
    "        }\n",
    "\n",
    "    def render(self):\n",
    "        if self.render_mode == \"rgb_array\":\n",
    "            return self._render_frame()\n",
    "\n",
    "    def _render_frame(self):\n",
    "        if self.window is None and self.render_mode == \"human\":\n",
    "            pygame.init()\n",
    "            pygame.display.init()\n",
    "            self.window = pygame.display.set_mode(\n",
    "                (self.window_size, self.window_size)\n",
    "            )\n",
    "        if self.clock is None and self.render_mode == \"human\":\n",
    "            self.clock = pygame.time.Clock()\n",
    "\n",
    "        canvas = pygame.Surface((self.window_size, self.window_size))\n",
    "        pix_square_size = (\n",
    "            self.window_size / self.size\n",
    "        )  # The size of a single grid square in pixels\n",
    "\n",
    "        # Background image\n",
    "        bg_img = pygame.image.load(self.metadata['background_img'])\n",
    "        bg_img.set_alpha(188)\n",
    "        bg_img = pygame.transform.scale(bg_img, (self.window_size, self.window_size))\n",
    "        canvas.blit(\n",
    "            bg_img,\n",
    "            [0, 0]\n",
    "        )\n",
    "\n",
    "        # Goal image\n",
    "        goal_img = pygame.image.load(self.metadata['goal_img'])\n",
    "        goal_img = pygame.transform.scale(goal_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            goal_img,\n",
    "            pix_square_size * self._goal_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Agent image\n",
    "        agent_img = pygame.image.load(self.metadata['agent_img'])\n",
    "        agent_img = pygame.transform.scale(agent_img, (pix_square_size, pix_square_size))\n",
    "        canvas.blit(\n",
    "            agent_img,\n",
    "            pix_square_size * self._agent_pos[::-1]\n",
    "        )\n",
    "\n",
    "        # Reward image\n",
    "        reward_img = pygame.image.load(self.metadata['reward_img'])\n",
    "        # Negative reward image\n",
    "        neg_reward_img = pygame.image.load(self.metadata['neg_reward_img'])\n",
    "\n",
    "        # Add the reward and neg reward\n",
    "        for x in range(self.size):\n",
    "            for y in range(self.size):\n",
    "                reward = self.state[x, y]\n",
    "                if self.state[x, y] > 0 and self.state[x, y] < 1:\n",
    "                    sreward_img = pygame.transform.scale(\n",
    "                        reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    rew_sz = np.array(sreward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= rew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sreward_img,\n",
    "                        position\n",
    "                    )\n",
    "                elif self.state[x, y] < 0:\n",
    "                    reward *= -1\n",
    "                    sneg_reward_img = pygame.transform.scale(\n",
    "                        neg_reward_img,\n",
    "                        (pix_square_size*reward, pix_square_size*reward)\n",
    "                    )\n",
    "                    nrew_sz = np.array(sneg_reward_img.get_size()) # w x h\n",
    "                    position = pix_square_size * (np.array([y, x])+0.5)\n",
    "                    # To center to grid square\n",
    "                    position -= nrew_sz[::-1] / 2\n",
    "                    canvas.blit(\n",
    "                        sneg_reward_img,\n",
    "                        position\n",
    "                    )\n",
    "\n",
    "        # Finally, add some gridlines\n",
    "        for x in range(self.size + 1):\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (0, pix_square_size * x),\n",
    "                (self.window_size, pix_square_size * x),\n",
    "                width=2,\n",
    "            )\n",
    "            pygame.draw.line(\n",
    "                canvas,\n",
    "                (255, 255, 255),\n",
    "                (pix_square_size * x, 0),\n",
    "                (pix_square_size * x, self.window_size),\n",
    "                width=2,\n",
    "            )\n",
    "\n",
    "        if self.render_mode == \"human\":\n",
    "            # The following line copies our drawings from `canvas` to the visible window\n",
    "            self.window.blit(canvas, canvas.get_rect())\n",
    "            pygame.event.pump()\n",
    "            pygame.display.update()\n",
    "\n",
    "            # We need to ensure that human-rendering occurs at the predefined framerate.\n",
    "            # The following line will automatically add a delay to keep the framerate stable.\n",
    "            self.clock.tick(self.metadata[\"render_fps\"])\n",
    "        else:  # rgb_array\n",
    "            return np.transpose(\n",
    "                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\n",
    "            )\n",
    "\n",
    "    def close(self):\n",
    "        if self.window is not None:\n",
    "            pygame.display.quit()\n",
    "            pygame.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent and the env\n",
    "senv = StocasticGridEnvironment(size=6, render_mode='rgb_array')\n",
    "\n",
    "# Check for API Compliance\n",
    "env_checker.check_env(senv)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**5\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "qlearning_agent = QLearningAgent(senv)\n",
    "qtrainer = QLearningTrainer(qlearning_agent)\n",
    "tr_q_history = qtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "qlearning_agent.save_pretrained('./q-table-sto.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# qlearning_agent.q_table = './q-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_episodes, 2500), tr_q_history[::2500], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_q_history = qtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_eval_episodes, 25), ev_q_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_policy = [senv._action_to_direction[qlearning_agent.q_table[i].argmax()]['name']\n",
    "                  for i in range(qlearning_agent.q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((senv.size, senv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "plt.rcParams[\"font.size\"] = 22\n",
    "\n",
    "obs, info = senv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(senv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = qlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = senv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(senv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "senv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Double Q-Learning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 10**4\n",
    "lr = .4\n",
    "gamma = 0.95\n",
    "ep_min = .08\n",
    "ep_max = 1.\n",
    "decay_rate = 8e-4\n",
    "\n",
    "dqlearning_agent = QLearningAgent(senv, doubleq=True)\n",
    "dqtrainer = DoubleQTrainer(dqlearning_agent)\n",
    "tr_dq_history = dqtrainer.train(num_episodes=num_episodes, gamma=gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Q Table\n",
    "dqlearning_agent.save_pretrained('./doubleq-table-sto.pkl')\n",
    "\n",
    "# The q table can be loaded from the pkl file by uncommenting the below line\n",
    "# dqlearning_agent.q_table = './doubleq-table-sto.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_episodes, 250), tr_dq_history[::250], label='Cumulative Reward per Episode', linewidth=2)\n",
    "plt.plot([ep_min+(ep_max-ep_min)*np.exp(-decay_rate*i) for i in range(num_episodes)], label='Explore Probability (Epsilon)')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_eval_episodes = 10**3\n",
    "ev_dq_history = dqtrainer.evaluate(num_episodes=num_eval_episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, num_eval_episodes, 25), ev_dq_history[::25],  linewidth=2)\n",
    "plt.ylabel('Cumulative Reward per Episode')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.grid(True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Learned Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the mean of each states action-value estimates\n",
    "mean_q_table =  dqlearning_agent.q_table.mean(axis=0)\n",
    "learned_policy = [senv._action_to_direction[mean_q_table[i].argmax()]['name']\n",
    "                  for i in range(mean_q_table.shape[0])]\n",
    "\n",
    "\n",
    "learned_policy = np.array(learned_policy).reshape((senv.size, senv.size))\n",
    "\n",
    "print(learned_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take the env for a spin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (20,20)\n",
    "\n",
    "obs, info = senv.reset(seed=88)\n",
    "\n",
    "terminated, truncated = False, False\n",
    "\n",
    "plt.imshow(senv.render())\n",
    "plt.show()\n",
    "\n",
    "ep_history = [({k: obs[k] for k in set(list(obs.keys())) - {'reward'}}, info)]\n",
    "while not (terminated or truncated):\n",
    "  action = dqlearning_agent.step(obs)\n",
    "  obs, reward, terminated, truncated, info = senv.step(action)\n",
    "\n",
    "  clear_output(wait=True)\n",
    "  plt.imshow(senv.render())\n",
    "  plt.show()\n",
    "\n",
    "  ep_history.append((\n",
    "    {k: obs[k] for k in set(list(obs.keys())) - {'reward'}},\n",
    "    info, reward))\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the simulation\n",
    "for item in ep_history:\n",
    "    print(item)\n",
    "\n",
    "senv.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(0, 10**4, 250), tr_dq_history[::250], label='Double Q-Learning', linewidth=2)\n",
    "plt.plot(range(0, 10**4, 250), tr_q_history[:10**4:250], label='Q-Learning', linewidth=2)\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Cumulative Reward per Episode\")\n",
    "plt.grid(True)\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('expt')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
