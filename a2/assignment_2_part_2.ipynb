{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Miniconda3\\envs\\expt\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count,\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, List, Union\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from gymnasium import Env\n",
    "from gymnasium import spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.pyplot._IonContext at 0x2549ebff2e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "    ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q-function approximator networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardDQN(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(inp_dim, 64)\n",
    "        self.output = nn.Linear(64, out_dim)\n",
    "\n",
    "        # Helps in creating clones\n",
    "        self.inp_args = [inp_dim, out_dim]\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Only if x is not already a tensor\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype=torch.float32)\n",
    "\n",
    "        x = F.relu(self.hidden(x))\n",
    "\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, inp_size, out_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        # Calculate the conv layers output\n",
    "        conv_op_dim = inp_size - 3 + 1\n",
    "\n",
    "        self.conv = nn.Conv2d(1, 128, 3)\n",
    "        self.maxpool = nn.MaxPool2d(conv_op_dim)\n",
    "\n",
    "        self.hidden = nn.Linear(128, 64)\n",
    "        self.output = nn.Linear(64, out_dim)\n",
    "\n",
    "        # Helps in creating clones\n",
    "        self.inp_args = [inp_size, out_dim]\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Convert to torch tensors\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            x = torch.as_tensor(state)\n",
    "\n",
    "        # Scale pixel values between 0 and 1\n",
    "        x = x/255 \n",
    "\n",
    "        # Handle add the channel dim\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.maxpool(x).flatten(start_dim=1)\n",
    "\n",
    "        x = self.hidden(F.relu(x))\n",
    "\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTrainer:\n",
    "    def __init__(self, env: Env, policy_net: nn.Module):\n",
    "        self.env = env\n",
    "        self.memory = ReplayMemory(int(1e5))\n",
    "\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.policy_net = policy_net.to(self.device)\n",
    "        # Clone the policy net\n",
    "        self.target_net = type(policy_net)(*policy_net.inp_args).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "       \n",
    "        # Cumulative reward of each episode\n",
    "        self.reward_history: List[float] = list()\n",
    "        # Number of time steps taken in each episode\n",
    "        self.episode_durations: Union[List[int], List[float]] = list()\n",
    "\n",
    "    def ep_greedy_policy(\n",
    "        self,\n",
    "        state,\n",
    "        *,\n",
    "        random_num: float,\n",
    "        epsilon: float\n",
    "    ):\n",
    "        if random_num > epsilon:\n",
    "            return self.greedy_policy(state)\n",
    "\n",
    "        else:\n",
    "            return torch.tensor(\n",
    "                [[self.env.action_space.sample()]],\n",
    "                device=self.device,\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "    def greedy_policy(self, state):\n",
    "        return self.policy_net(state).argmax(1).reshape(1, 1)\n",
    "\n",
    "    def _optimize(self, lr: float, gamma: float, batch_size: int):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=lr,\n",
    "            amsgrad=True\n",
    "        )\n",
    "\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        # Transpose the batch\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device, dtype=torch.bool\n",
    "        )\n",
    "\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        )\n",
    "\n",
    "        state_batch = torch.cat(batch.state)\n",
    "        action_batch = torch.cat(batch.action)\n",
    "        reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "        # Get the policy network's estimations of the current state's action values.\n",
    "        st_action_values  = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Get the target network's estimations of the next state's action values.\n",
    "        nx_st_action_values = torch.zeros(self.batch_size, device=self.device)\n",
    "        with torch.no_grad():\n",
    "            nx_st_action_values[non_final_mask] = \\\n",
    "                self.target_net(non_final_next_states).max(1)[0]\n",
    "\n",
    "        target_st_action_values = (nx_st_action_values*gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        # We use Huber loss instead of MSE as it gives better stability\n",
    "        # It behaves like MSE when the error is small and mae when it is large\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(st_action_values, target_st_action_values.unsqueeze(1))\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        optimizer.step()\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        *,\n",
    "        num_episodes: int = 600,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = .99,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = 0.95,\n",
    "        decay_rate: float = .0008,\n",
    "        tau: float = 0.005,\n",
    "        batch_size: int = 16,\n",
    "        quiet: bool = False\n",
    "    ) -> List[float]:\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 600.\n",
    "        lr (float, optional): The learning rate for the otimizer. Defaults to 1e-4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to .0008.\n",
    "        tau (float, optional): The update rate of the target network\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        for episode in tqdm(range(num_episodes), desc='Train Episode', disable=quiet):\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            state, _ = self.env.reset(seed=88)\n",
    "            state = torch.tensor(\n",
    "                state, dtype=torch.float32, device=self.device\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "            # Run the episode\n",
    "            for t in count():\n",
    "                action = self.ep_greedy_policy(state, epsilon)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(\n",
    "                        next_state, dtype=torch.float32, device=self.device\n",
    "                    ).unsqueeze(0)\n",
    "\n",
    "                total_reward += reward\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                # Otimize the DQN\n",
    "                self._optimize(lr, gamma, batch_size)\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = \\\n",
    "                        policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    self.plot_durations()\n",
    "                    self.reward_history.append(total_reward)\n",
    "                    self.plot_rewards()\n",
    "                    break\n",
    "\n",
    "    def evaluate(self, num_episodes: int = 10**2, quiet: bool = False) -> List[float]:\n",
    "        \"\"\"Evaluate the agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): Defaults to 10**2.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        for _ in tqdm(range(num_episodes), desc='Eval Episode: ', disable=quiet):\n",
    "            total_reward = 0\n",
    "            state, _ = self.env.reset(seed=88)\n",
    "            state = torch.tensor(\n",
    "                state, dtype=torch.float32, device=self.device\n",
    "            ).unsqueeze(0)\n",
    "\n",
    "            # Run the episode\n",
    "            for t in count():\n",
    "                action = self.greedy_policy(state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([reward], device=self.device)\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(\n",
    "                        next_state, dtype=torch.float32, device=self.device\n",
    "                    ).unsqueeze(0)\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t + 1)\n",
    "                    self.plot_durations()\n",
    "                    self.reward_history.append(total_reward)\n",
    "                    self.plot_rewards()\n",
    "                    break\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Env Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a1_env import DeterministicGridEnvironment\n",
    "\n",
    "class A1EnvWrapper(DeterministicGridEnvironment):\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = super().reset(seed, options)\n",
    "\n",
    "        state = obs['agent']\n",
    "        # Convert 2-d state index to flat index\n",
    "        # flat_idx = curr_row*num_cols+curr_col\n",
    "        state = torch.tensor(state[0]*self.env.size+state[1])\n",
    "        # Convert to one-hot form\n",
    "        state = F.one_hot(\n",
    "            state,\n",
    "            num_classes=self.size**2\n",
    "        ).to(torch.float32)\n",
    "\n",
    "        # The reward situation\n",
    "        situation = torch.from_numpy(obs['reward'])\n",
    "\n",
    "        # Concatenate the state and situation\n",
    "        state = torch.cat((state, situation))\n",
    "\n",
    "        return state, info\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        state = obs['agent']\n",
    "        # Convert 2-d state index to flat index\n",
    "        # flat_idx = curr_row*num_cols+curr_col\n",
    "        state = torch.tensor(state[0]*self.env.size+state[1])\n",
    "        # Convert to one-hot form\n",
    "        state = F.one_hot(\n",
    "            state,\n",
    "            num_classes=self.size**2\n",
    "        ).to(torch.float32)\n",
    "\n",
    "        # The reward situation\n",
    "        situation = torch.from_numpy(obs['reward'])\n",
    "\n",
    "        # Concatenate the state and situation\n",
    "        state = torch.cat((state, situation))\n",
    "\n",
    "        return state, reward, terminated, truncated, info\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
