{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from itertools import count\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Any, List, Union\n",
    "from collections import namedtuple, deque, OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import Env\n",
    "from gymnasium.wrappers import (\n",
    "    GrayScaleObservation,\n",
    "    PixelObservationWrapper,\n",
    "    TransformObservation\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up matplotlib\n",
    "is_ipython = 'inline' in matplotlib.get_backend()\n",
    "if is_ipython:\n",
    "    from IPython import display\n",
    "\n",
    "plt.ion()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "    ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Q-function approximator networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardDQN(nn.Module):\n",
    "    def __init__(self, inp_dim, out_dim):\n",
    "        super().__init__()\n",
    "        self.hidden1 = nn.Linear(inp_dim, 128)\n",
    "        self.hidden2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, out_dim)\n",
    "\n",
    "        # Helps in creating clones\n",
    "        self.inp_args = [inp_dim, out_dim]\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Only if x is not already a tensor\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.tensor(state, dtype=torch.float32)\n",
    "\n",
    "        x = F.relu(self.hidden1(state))\n",
    "        x = F.relu(self.hidden2(x))\n",
    "\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDQN(nn.Module):\n",
    "    def __init__(self, inp_shape, out_dim):\n",
    "        super().__init__()\n",
    "        self.inp_shape = inp_shape\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(inp_shape[1], 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(64, 64, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(64, 16, 3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(3),\n",
    "            nn.Dropout(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "\n",
    "        conv_op_shape = self._conv_op_shape()\n",
    "\n",
    "        self.hidden = nn.Linear(conv_op_shape, int(conv_op_shape/2))\n",
    "        self.output = nn.Linear(int(conv_op_shape/2), out_dim)\n",
    "\n",
    "        # Helps in creating clones\n",
    "        self.inp_args = [inp_shape, out_dim]\n",
    "\n",
    "    def _conv_op_shape(self):\n",
    "        with torch.no_grad():\n",
    "            x = torch.zeros(\n",
    "                self.inp_shape,\n",
    "                device=next(self.parameters()).device\n",
    "            )\n",
    "            x = self.conv(x)\n",
    "        return x.shape[-1]\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Convert to torch tensors\n",
    "        if not isinstance(state, torch.Tensor):\n",
    "            state = torch.as_tensor(state)\n",
    "\n",
    "        # Scale pixel values between 0 and 1\n",
    "        x = state/255 \n",
    "\n",
    "        # Handle add the channel dim\n",
    "        if len(x.shape) == 3:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        x = self.conv(x)\n",
    "        x = self.hidden(F.relu(x))\n",
    "\n",
    "        return self.output(x)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Q-Learning Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningTrainer(object):\n",
    "    def __init__(self, env: Env, policy_net: nn.Module):\n",
    "        self.env = env\n",
    "\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.policy_net = policy_net.to(self.device)\n",
    "        # Clone the policy net\n",
    "        self.target_net = type(policy_net)(*policy_net.inp_args).to(self.device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "       \n",
    "        # Cumulative reward of each episode\n",
    "        self.reward_history: List[float] = list()\n",
    "        # Number of time steps taken in each episode\n",
    "        self.episode_durations: Union[List[int], List[float]] = list()\n",
    "        # Record Epsilon values\n",
    "        self.epsilon_vals: List[float] = list()\n",
    "\n",
    "    def ep_greedy_policy(\n",
    "        self,\n",
    "        state,\n",
    "        *,\n",
    "        epsilon: float\n",
    "    ):\n",
    "        if self.env.np_random.uniform(0, 1) > epsilon:\n",
    "            return self.greedy_policy(state)\n",
    "\n",
    "        else:\n",
    "            return torch.tensor(\n",
    "                [[self.env.action_space.sample()]],\n",
    "                dtype=torch.long\n",
    "            )\n",
    "\n",
    "    def greedy_policy(self, state):\n",
    "        with torch.no_grad():\n",
    "            return self.policy_net(\n",
    "                state.to(self.device)\n",
    "            ).max(1)[1].reshape(1, 1).cpu()\n",
    "\n",
    "    def _optimize(self, lr: float, gamma: float, batch_size: int):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        # Transpose the batch\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device, dtype=torch.bool\n",
    "        )\n",
    "\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "\n",
    "        # Get the policy network's estimations of the current state's action values.\n",
    "        st_action_values  = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        # Get the target network's estimations of the next state's action values.\n",
    "        nx_st_action_values = torch.zeros((batch_size, 1), device=self.device)\n",
    "        with torch.no_grad():\n",
    "            nx_st_action_values[non_final_mask] = \\\n",
    "                self.target_net(non_final_next_states).max(1)[0].unsqueeze(1)\n",
    "\n",
    "        target_st_action_values = (nx_st_action_values*gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        # We use Huber loss instead of MSE as it gives better stability\n",
    "        # It behaves like MSE when the error is small and mae when it is large\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(st_action_values, target_st_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    @classmethod\n",
    "    def _transform_state(cls, state):\n",
    "        if not isinstance(state, OrderedDict): return state\n",
    "        else:\n",
    "            state = cv2.resize(state['pixels'], dsize=(84, 84), interpolation=cv2.INTER_CUBIC)\n",
    "            state = np.expand_dims(state, 0)\n",
    "            return np.moveaxis(state, -1, 1)\n",
    "\n",
    "    def train(\n",
    "        self,\n",
    "        memory: ReplayMemory,\n",
    "        *,\n",
    "        num_episodes: int = 600,\n",
    "        lr: float = 1e-4,\n",
    "        gamma: float = .99,\n",
    "        ep_min: float = .08,\n",
    "        ep_max: float = .95,\n",
    "        decay_rate: float = 8e-3,\n",
    "        tau: float = 5e-3,\n",
    "        batch_size: int = 16,\n",
    "        quiet: bool = False\n",
    "    ):\n",
    "        \"\"\"Train the agent\n",
    "\n",
    "        Args:\n",
    "        num_episodes (int, optional): Defaults to 600.\n",
    "        lr (float, optional): The learning rate for the otimizer. Defaults to 1e-4.\n",
    "        gamma (float, optional): The discount factor. Defaults to .9.\n",
    "        ep_min (float, optional): Min exploration probability. Defaults to .08.\n",
    "        ep_max (float, optional): Max exploration probability. Defaults to 1.\n",
    "        decay_rate (float, optional): Epsilon decay rate. Defaults to 8*1e-3.\n",
    "        tau (float, optional): The update rate of the target network. Defaults to .005\n",
    "\n",
    "        Returns:\n",
    "        List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        self.memory = memory\n",
    "        self.optimizer = optim.AdamW(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=lr,\n",
    "            amsgrad=True\n",
    "        )\n",
    "\n",
    "        prog_bar = tqdm(range(num_episodes), desc='Train Episode', disable=quiet)\n",
    "        for episode in prog_bar:\n",
    "            epsilon = ep_min + (ep_max-ep_min)*np.exp(-decay_rate*episode)\n",
    "\n",
    "            total_reward = 0\n",
    "            state, _ = self.env.reset(seed=88)\n",
    "            state = torch.tensor(\n",
    "                self._transform_state(state), dtype=torch.float32\n",
    "            ) # no need to unsqueeze, the state is rank 2 already\n",
    "\n",
    "            # Run the episode\n",
    "            for t in count():\n",
    "                action = self.ep_greedy_policy(\n",
    "                    state,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([[reward]])\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(\n",
    "                        self._transform_state(next_state), dtype=torch.float32\n",
    "                    ) # no need to unsqueeze, the state is rank 2 already\n",
    "\n",
    "                total_reward += reward\n",
    "                # Store the transition in memory\n",
    "                self.memory.push(state, action, next_state, reward)\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                # Otimize the DQN\n",
    "                self._optimize(lr, gamma, batch_size)\n",
    "\n",
    "                # Soft update of the target network's weights\n",
    "                # θ′ ← τ θ + (1 −τ )θ′\n",
    "                target_net_state_dict = self.target_net.state_dict()\n",
    "                policy_net_state_dict = self.policy_net.state_dict()\n",
    "\n",
    "                for key in policy_net_state_dict:\n",
    "                    target_net_state_dict[key] = \\\n",
    "                        policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
    "\n",
    "                self.target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "                if done:\n",
    "                    self.epsilon_vals.append(epsilon)\n",
    "                    self.episode_durations.append(t+1)\n",
    "                    # self.plot_durations()\n",
    "                    self.reward_history.append(total_reward.item())\n",
    "                    # self.plot_rewards()\n",
    "                    prog_bar.set_postfix_str(\n",
    "                        f\"episode_duration: {t+1}; cumulative_reward: {total_reward.item()}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "    def evaluate(self, num_episodes: int = 10**2, quiet: bool = False):\n",
    "        \"\"\"Evaluate the agent\n",
    "\n",
    "        Args:\n",
    "            num_episodes (int): Defaults to 10**2.\n",
    "\n",
    "        Returns:\n",
    "            List[float]: Reward earned in every episode.\n",
    "        \"\"\"\n",
    "        prog_bar = tqdm(range(num_episodes), desc='Eval Episode: ', disable=quiet)\n",
    "        for _ in prog_bar:\n",
    "            total_reward = 0\n",
    "            state, _ = self.env.reset(seed=88)\n",
    "            state = torch.tensor(\n",
    "                self._transform_state(state), dtype=torch.float32\n",
    "            ) # no need to unsqueeze, the state is rank 2 already\n",
    "\n",
    "            # Run the episode\n",
    "            for t in count():\n",
    "                action = self.greedy_policy(state)\n",
    "\n",
    "                next_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                reward = torch.tensor([[reward]])\n",
    "                done = terminated or truncated\n",
    "\n",
    "                if terminated:\n",
    "                    next_state = None\n",
    "                else:\n",
    "                    next_state = torch.tensor(\n",
    "                        self._transform_state(next_state), dtype=torch.float32\n",
    "                    ) # no need to unsqueeze, the state is rank 2 already\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                if done:\n",
    "                    self.episode_durations.append(t+1)\n",
    "                    # self.plot_metrics()\n",
    "                    self.reward_history.append(total_reward)\n",
    "                    prog_bar.set_postfix_str(\n",
    "                        f\"episode_duration: {t+1}; cumulative_reward: {total_reward.item()}\"\n",
    "                    )\n",
    "                    break\n",
    "\n",
    "    def plot_metrics(self, show_result=False):\n",
    "        durations_t = torch.tensor(self.episode_durations, dtype=torch.float)\n",
    "        rewards_t = torch.tensor(self.reward_history, dtype=torch.float)\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (30,12)\n",
    "        plt.rcParams[\"font.size\"] = 22\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "        ax1.set_xlabel('Episode')\n",
    "        ax1.set_ylabel('Duration (in Time Steps)')\n",
    "        ax1.plot(durations_t.numpy(), label='Duration', linewidth=2)\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(durations_t) >= 100:\n",
    "            means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            ax1.plot(means.numpy(), label='Running Average Duration', linewidth=2)\n",
    "        ax1.plot(self.epsilon_vals, label='Epsilon', linewidth=2)\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.set_xlabel('Episode')\n",
    "        ax2.set_ylabel('Cumulative Reward (per Episode)')\n",
    "        ax2.plot(rewards_t.numpy(), label='Reward', linewidth=2)\n",
    "        # Take 100 episode averages and plot them too\n",
    "        if len(rewards_t) >= 100:\n",
    "            means = rewards_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "            means = torch.cat((torch.zeros(99), means))\n",
    "            ax2.plot(means.numpy(), label='Running Average Reward', linewidth=2)\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "        if is_ipython:\n",
    "            if not show_result:\n",
    "                display.display(plt.gcf())\n",
    "                display.clear_output(wait=True)\n",
    "            else:\n",
    "                display.display(plt.gcf())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the Env Wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from a1_env import DeterministicGridEnvironment\n",
    "\n",
    "class A1EnvWrapper(DeterministicGridEnvironment):\n",
    "    def reset(self, seed=None, options=None):\n",
    "        obs, info = super().reset(seed, options)\n",
    "\n",
    "        state = obs['agent']\n",
    "        # Convert 2-d state index to flat index\n",
    "        # flat_idx = curr_row*num_cols+curr_col\n",
    "        state = torch.tensor([state[0]*self.size+state[1]], dtype=torch.long)\n",
    "\n",
    "        # The reward situation\n",
    "        situation = torch.from_numpy(obs['reward']).unsqueeze(0)\n",
    "\n",
    "        # Set the `state`-th element of the `situation` tensor at 0.5\n",
    "        situation[0, state] = .5\n",
    "\n",
    "        return situation, info\n",
    "        \n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = super().step(action)\n",
    "\n",
    "        state = obs['agent']\n",
    "        # Convert 2-d state index to flat index\n",
    "        # flat_idx = curr_row*num_cols+curr_col\n",
    "        state = torch.tensor([state[0]*self.size+state[1]], dtype=torch.long)\n",
    "\n",
    "        # The reward situation\n",
    "        situation = torch.from_numpy(obs['reward']).unsqueeze(0)\n",
    "\n",
    "        # Set the `state`-th element of the `situation` tensor at 0.5\n",
    "        situation[0, state] = .5\n",
    "\n",
    "        return situation, reward, terminated, truncated, info\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DQN Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent and the env\n",
    "a1env = A1EnvWrapper(size=6, render_mode='rgb_array', max_time_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(int(1e4))\n",
    "policy_net = FeedForwardDQN(a1env.size**2, a1env.action_space.n)\n",
    "qtrainer = QLearningTrainer(a1env, policy_net)\n",
    "\n",
    "qtrainer.train(memory, num_episodes=2048, batch_size=64, lr=1e-3, tau=5e-4)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer = QLearningTrainer(a1env, policy_net)\n",
    "qtrainer.evaluate(num_episodes=1000)\n",
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole-v1 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartenv = gym.make(\"CartPole-v1\")\n",
    "# Transform the obs space\n",
    "cartenv = TransformObservation(cartenv, lambda obs: np.expand_dims(obs, axis=0))\n",
    "\n",
    "policy_net = FeedForwardDQN(cartenv.observation_space.shape[0], cartenv.action_space.n)\n",
    "qtrainer = QLearningTrainer(cartenv, policy_net)\n",
    "\n",
    "memory = ReplayMemory(int(1e4))\n",
    "qtrainer.train(memory, num_episodes=2048, lr=1e-3, batch_size=64, tau=5e-4)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer = QLearningTrainer(cartenv, policy_net)\n",
    "qtrainer.evaluate(num_episodes=1000)\n",
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Double Q Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleQLearningTrainer(QLearningTrainer):\n",
    "    def _optimize(self, lr: float, gamma: float, batch_size: int):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        optimizer = optim.AdamW(\n",
    "            self.policy_net.parameters(),\n",
    "            lr=lr,\n",
    "            amsgrad=True\n",
    "        )\n",
    "\n",
    "        transitions = self.memory.sample(batch_size)\n",
    "        # Transpose the batch\n",
    "        batch = Transition(*zip(*transitions))\n",
    "\n",
    "        # Compute a mask of non-final states and concatenate the batch elements\n",
    "        non_final_mask = torch.tensor(\n",
    "            tuple(map(lambda s: s is not None, batch.next_state)),\n",
    "            device=self.device, dtype=torch.bool\n",
    "        )\n",
    "\n",
    "        non_final_next_states = torch.cat(\n",
    "            [s for s in batch.next_state if s is not None]\n",
    "        ).to(self.device)\n",
    "\n",
    "        state_batch = torch.cat(batch.state).to(self.device)\n",
    "        action_batch = torch.cat(batch.action).to(self.device)\n",
    "        reward_batch = torch.cat(batch.reward).to(self.device)\n",
    "\n",
    "        # Get the policy network's estimations of the current state's action values.\n",
    "        st_action_values  = self.policy_net(state_batch).gather(1, action_batch)\n",
    "\n",
    "        nx_st_action_values = torch.zeros((batch_size, 1), device=self.device)\n",
    "        with torch.no_grad():\n",
    "            # Get the maximum value action based on the `policy network`\n",
    "            nx_st_policy_net_actions = self.policy_net(\n",
    "                non_final_next_states\n",
    "            ).max(1)[1].unsqueeze(1)\n",
    "            # Get the target network's estimations of the next state's action values.\n",
    "            nx_st_action_values[non_final_mask] = \\\n",
    "                self.target_net(non_final_next_states).gather(\n",
    "                    1, nx_st_policy_net_actions\n",
    "                )\n",
    "\n",
    "        target_st_action_values = (nx_st_action_values*gamma) + reward_batch\n",
    "\n",
    "        # Compute Huber loss\n",
    "        # We use Huber loss instead of MSE as it gives better stability\n",
    "        # It behaves like MSE when the error is small and mae when it is large\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(st_action_values, target_st_action_values)\n",
    "\n",
    "        # Optimize the model\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # In-place gradient clipping\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train DDQN Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid World Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the agent and the env\n",
    "a1env = A1EnvWrapper(size=6, render_mode='rgb_array', max_time_steps=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(int(1e4))\n",
    "policy_net = FeedForwardDQN(a1env.size**2, a1env.action_space.n)\n",
    "qtrainer = DoubleQLearningTrainer(a1env, policy_net)\n",
    "\n",
    "qtrainer.train(memory, num_episodes=2048, batch_size=64, lr=1e-3, tau=5e-4)\n",
    "\n",
    "print('Complete')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer = DoubleQLearningTrainer(a1env, policy_net)\n",
    "qtrainer.evaluate(num_episodes=1000)\n",
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole-v1 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cartenv = gym.make(\"CartPole-v1\")\n",
    "# Transform the obs space\n",
    "cartenv = TransformObservation(cartenv, lambda obs: np.expand_dims(obs, axis=0))\n",
    "\n",
    "policy_net = FeedForwardDQN(cartenv.observation_space.shape[0], cartenv.action_space.n)\n",
    "qtrainer = DoubleQLearningTrainer(cartenv, policy_net)\n",
    "\n",
    "memory = ReplayMemory(int(1e4))\n",
    "qtrainer.train(memory, num_episodes=2048, lr=1e-3, batch_size=64, tau=5e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qtrainer = DoubleQLearningTrainer(cartenv, policy_net)\n",
    "qtrainer.evaluate(num_episodes=1000)\n",
    "qtrainer.plot_metrics(show_result=True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c7d4cfcaae307a7383f424736b9bb7961522145721c6eef7a7425780af807757"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
