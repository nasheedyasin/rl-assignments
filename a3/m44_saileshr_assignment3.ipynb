{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing Stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "from tqdm import tqdm\n",
    "from itertools import count\n",
    "from typing import List, Union"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the actor and Critic Nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QApproximator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, int(hidden_dim/2))\n",
    "        self.fc3 = nn.Linear(int(hidden_dim/2), output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_values = self.fc3(x)\n",
    "        return q_values\n",
    "\n",
    "\n",
    "class PolicyNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim=64):\n",
    "        super(PolicyNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, int(hidden_dim/2))\n",
    "        self.fc3 = nn.Linear(int(hidden_dim/2), output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        actor_prob_logits = self.fc3(x)\n",
    "        return actor_prob_logits\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the Q-Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseACAgent(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        env: gym.Env,\n",
    "        policy_net: nn.Module,\n",
    "        critic_net: nn.Module,\n",
    "        use_cuda: bool = False\n",
    "    ):\n",
    "        self.env = env\n",
    "        # if gpu is to be used\n",
    "        self.device = torch.device(\"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        self.policy_net = policy_net.to(self.device)\n",
    "        self.critic_net = critic_net.to(self.device)\n",
    "\n",
    "        # Cumulative reward of each episode\n",
    "        self.tr_rewards: List[float] = list()\n",
    "        self.ev_rewards: List[float] = list()\n",
    "        self.tr_avg_rewards: List[float] = list()\n",
    "        self.tr_avg_duration: Union[List[int], List[float]] = list()\n",
    "        # Number of time steps taken in each episode\n",
    "        self.tr_durations: Union[List[int], List[float]] = list()\n",
    "        self.ev_durations: Union[List[int], List[float]] = list()\n",
    "        self.ev_avg_rewards: List[float] = list()\n",
    "        self.ev_avg_duration: Union[List[int], List[float]] = list()\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        actor_prob_logits = self.policy_net(state)\n",
    "        dist = Categorical(logits=actor_prob_logits)\n",
    "        action = dist.sample()\n",
    "        action_log_prob = dist.log_prob(action)\n",
    "\n",
    "        return action, action_log_prob\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        num_episodes: int = 150,\n",
    "        report_every_n_episodes: int = 10,\n",
    "        quiet: bool = False\n",
    "    ):\n",
    "        prog_bar = tqdm(range(num_episodes), desc='Evaluation Episode', disable=quiet)\n",
    "        for episode in prog_bar:\n",
    "            ep_reward = torch.tensor(0.)\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "            for steps in count():\n",
    "                with torch.no_grad():\n",
    "                    action, _ = self.choose_action(state)\n",
    "\n",
    "                    new_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                    new_state = torch.from_numpy(new_state).float().to(self.device)\n",
    "\n",
    "                    ep_reward += reward\n",
    "                    state = new_state\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    self.ev_rewards.append(ep_reward)\n",
    "                    self.ev_durations.append(steps)\n",
    "                    self.ev_avg_rewards.append(np.mean(self.ev_rewards[-report_every_n_episodes:]))\n",
    "                    self.ev_avg_duration.append(np.mean(self.ev_durations[-report_every_n_episodes:]))\n",
    "                    if episode % report_every_n_episodes == 0:\n",
    "                        prog_bar.set_postfix_str(\n",
    "                            f\"{report_every_n_episodes}-episode average reward: {self.ev_avg_rewards[-1]}, \"\\\n",
    "                            f\"{report_every_n_episodes}-episode average len: {self.ev_avg_duration[-1]}\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "    def plot_performance(self):\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.rcParams[\"figure.figsize\"] = (60,28)\n",
    "        plt.rcParams[\"font.size\"] = 44\n",
    "        \n",
    "        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2)\n",
    "\n",
    "        ax1.set_xlabel('Training Episodes')\n",
    "        ax1.set_ylabel('Running Duration')\n",
    "        ax1.plot(self.tr_durations, linewidth=6, label='Per Episode', color='#0077b6')\n",
    "        ax1.plot(self.tr_avg_duration, linewidth=6, label='Running Average', color='#420c09')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True)\n",
    "\n",
    "        ax2.set_xlabel('Training Episodes')\n",
    "        ax2.set_ylabel('Epiosde Reward')\n",
    "        ax2.plot(self.tr_rewards, linewidth=6, label='Per Episode', color='#0077b6')\n",
    "        ax2.plot(self.tr_avg_rewards, linewidth=6, label='Running Average', color='#420c09')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True)\n",
    "\n",
    "        ax3.set_xlabel('Evaluation Episodes')\n",
    "        ax3.set_ylabel('Running Duration')\n",
    "        ax3.plot(self.ev_durations, linewidth=6, label='Per Episode', color='#0077b6')\n",
    "        ax3.plot(self.ev_avg_duration, linewidth=6, label='Running Average', color='#420c09')\n",
    "        ax3.legend()\n",
    "        ax3.grid(True)\n",
    "\n",
    "        ax4.set_xlabel('Evaluation Episodes')\n",
    "        ax4.set_ylabel('Epiosde Reward')\n",
    "        ax4.plot(self.ev_rewards, linewidth=6, label='Per Episode', color='#0077b6')\n",
    "        ax4.plot(self.ev_avg_rewards, linewidth=6, label='Running Average', color='#420c09')\n",
    "        ax4.legend()\n",
    "        ax4.grid(True)\n",
    "\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QACAgent(BaseACAgent):\n",
    "    def train(\n",
    "        self,\n",
    "        ac_optimizer: optim.Optimizer,\n",
    "        cr_optimizer: optim.Optimizer,\n",
    "        num_episodes: int = int(1e3),\n",
    "        disc_factor: float = .99,\n",
    "        report_every_n_episodes: int = 30,\n",
    "        quiet: bool = False,\n",
    "        early_stopping_rwd: float = float('inf'),\n",
    "        early_stopping_window: int = 5\n",
    "    ):\n",
    "        sheduler_args = {\n",
    "            'milestones': [int(num_episodes/3), int(3*num_episodes/4)],\n",
    "            'gamma': .5\n",
    "        }\n",
    "        ac_scheduler = optim.lr_scheduler.MultiStepLR(ac_optimizer, **sheduler_args)\n",
    "        cr_scheduler = optim.lr_scheduler.MultiStepLR(ac_optimizer, **sheduler_args)\n",
    "\n",
    "        prog_bar = tqdm(range(num_episodes), desc='Training Episode', disable=quiet)\n",
    "        for episode in prog_bar:\n",
    "            ep_reward = torch.tensor(0.)\n",
    "\n",
    "            state, _ = self.env.reset()\n",
    "            state = torch.from_numpy(state).float().to(self.device)\n",
    "            for steps in count():\n",
    "                action, log_prob = self.choose_action(state)\n",
    "\n",
    "                new_state, reward, terminated, truncated, _ = self.env.step(action.item())\n",
    "                new_state = torch.from_numpy(new_state).float().to(self.device)                \n",
    "                ep_reward += reward\n",
    "\n",
    "                next_action, _ = self.choose_action(new_state)\n",
    "\n",
    "                # Get Q values\n",
    "                qvalue = self.critic_net(state)[action.item()]\n",
    "                next_qvalue = self.critic_net(new_state)[next_action.item()]\n",
    "\n",
    "                q_target = reward + disc_factor * next_qvalue * int(not terminated)\n",
    "\n",
    "                # Critic loss\n",
    "                criterion = nn.SmoothL1Loss()\n",
    "                critic_loss = criterion(qvalue, q_target)\n",
    "\n",
    "                # Actor loss\n",
    "                actor_loss = -log_prob * (q_target-qvalue).detach()\n",
    "\n",
    "                ac_optimizer.zero_grad()\n",
    "                actor_loss.backward()\n",
    "                # In-place gradient clipping to prevent exploding gradients.\n",
    "                # torch.nn.utils.clip_grad_value_(self.actor_net.parameters(), 10)\n",
    "                ac_optimizer.step()\n",
    "\n",
    "                cr_optimizer.zero_grad()\n",
    "                critic_loss.backward()\n",
    "                # In-place gradient clipping to prevent exploding gradients.\n",
    "                # torch.nn.utils.clip_grad_value_(self.critic_net.parameters(), 10)\n",
    "                cr_optimizer.step()\n",
    "\n",
    "                # Move ahead\n",
    "                state = new_state\n",
    "\n",
    "                if terminated or truncated:\n",
    "                    self.tr_rewards.append(ep_reward)\n",
    "                    self.tr_durations.append(steps)\n",
    "                    self.tr_avg_rewards.append(np.mean(self.tr_rewards[-report_every_n_episodes:]))\n",
    "                    self.tr_avg_duration.append(np.mean(self.tr_durations[-report_every_n_episodes:]))\n",
    "                    if episode % report_every_n_episodes == 0:\n",
    "                        prog_bar.set_postfix_str(\n",
    "                            f\"{report_every_n_episodes}-episode average reward: {self.tr_avg_rewards[-1]}, \"\\\n",
    "                            f\"{report_every_n_episodes}-episode average len: {self.tr_avg_duration[-1]}\"\n",
    "                        )\n",
    "                    break\n",
    "\n",
    "            # Stop training if the last `early_stopping_window` reward\n",
    "            # running averages are above `early_stopping_rwd`\n",
    "            if all([rwd > early_stopping_rwd\n",
    "                for rwd in self.tr_avg_rewards[-early_stopping_window:]]):\n",
    "                    break\n",
    "\n",
    "            ac_scheduler.step()\n",
    "            cr_scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: CartPole-v1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions Q Actor-Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 64\n",
    "ac_learning_rate = 1e-3\n",
    "cr_learning_rate = 1e-3\n",
    "\n",
    "cartenv = gym.make('CartPole-v1')\n",
    "\n",
    "num_inputs = cartenv.observation_space.shape[0]\n",
    "num_outputs = cartenv.action_space.n\n",
    "\n",
    "actor = PolicyNet(num_inputs, num_outputs, hidden_size)\n",
    "critic = QApproximator(num_inputs, num_outputs, hidden_size)\n",
    "\n",
    "ac_optimizer = optim.AdamW(actor.parameters(), lr=ac_learning_rate)\n",
    "cr_optimizer = optim.AdamW(critic.parameters(), lr=cr_learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_agent = QACAgent(cartenv, actor, critic, use_cuda=False)\n",
    "\n",
    "qac_agent.train(ac_optimizer, cr_optimizer, num_episodes=2048, early_stopping_rwd=475)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_agent.evaluate(num_episodes=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_agent.plot_performance()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2.1: LunarLander-v2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions Q Actor Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 64\n",
    "ac_learning_rate = 1e-3\n",
    "cr_learning_rate = 1e-3\n",
    "\n",
    "lunarenv = gym.make('LunarLander-v2')\n",
    "\n",
    "num_inputs = lunarenv.observation_space.shape[0]\n",
    "num_outputs = lunarenv.action_space.n\n",
    "\n",
    "actor = PolicyNet(num_inputs, num_outputs, hidden_size)\n",
    "critic = QApproximator(num_inputs, num_outputs, hidden_size)\n",
    "\n",
    "ac_optimizer = optim.AdamW(actor.parameters(), lr=ac_learning_rate)\n",
    "cr_optimizer = optim.AdamW(critic.parameters(), lr=cr_learning_rate)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_agent = QACAgent(lunarenv, actor, critic, use_cuda=True)\n",
    "\n",
    "qac_agent.train(ac_optimizer, cr_optimizer, num_episodes=2**13, early_stopping_rwd=475)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_agent.evaluate(num_episodes=200)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qac_agent.plot_performance()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "expts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
